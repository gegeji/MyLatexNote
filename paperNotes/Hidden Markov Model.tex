\chapter{Hidden Markov Model}

\section{隐马尔可夫模型定义}

隐马尔可夫模型是关于时序的概率模型，描
述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生
成一个观测而产生观测随机序列的过程，隐藏的马尔可夫链随机生成的状态的序
列，称为状态序列(\term{state sequence});每个状态生成一个观测，而由此产生的观测
的随机序列，称为观测序列(\term{observation sequence}).序列的每一个位置又可以看
作是一个时刻。 

\begin{definition}[隐马尔可夫模型]
    设 $ Q $ 是所有可能的状态的集合， $ V $ 是所有可能的观测的集合。

\begin{equation}
Q=\left\{q_{1}, q_{2}, \cdots, q_{N}\right\}, \quad V=\left\{v_{1}, v_{2}, \cdots, v_{M}\right\}
\end{equation}

$ I $ 是长度为 $ T $ 的状态序列， $ O $ 是对应的观测序列。
\begin{equation}
I=\left(i_{1}, i_{2}, \cdots, i_{T}\right), \quad O=\left(o_{1}, o_{2}, \cdots, o_{T}\right)
\end{equation}

$ A $ 是状态转移概率矩阵:
\begin{equation}
A=\left[a_{i j}\right]_{N \times N}
\end{equation}
其中，
\begin{equation}
a_{i j}=P\left(i_{t+1}=q_{j} \mid i_{t}=q_{i}\right), \quad i=1,2, \cdots, N ; j=1,2, \cdots, N
\end{equation}
是在时刻 $ t $ 处于状态 $ q_{i} $ 的条件下在时刻 $ t+1 $ 转移到状态 $ q_{j} $ 的概率。

$ B $ 是观测概率矩阵:
\begin{equation}
B=\left[b_{j}(k)\right]_{N \times M}
\end{equation}
其中，
\begin{equation}
b_{j}(k)=P\left(o_{t}=v_{k} \mid i_{t}=q_{j}\right), \quad k=1,2, \cdots, M ; j=1,2, \cdots, N
\end{equation}
是在时刻 $ t $ 处于状态 $ q_{j} $ 的条件下生成观测 $ v_{k} $ 的概率。

$ \pi $ 是初始状态概率向量:
\begin{equation}
\pi=\left(\pi_{i}\right)
\end{equation}
其中，
\begin{equation}
\pi_{i}=P\left(i_{1}=q_{i}\right), \quad i=1,2, \cdots, N
\end{equation}
是时刻 $ t=1 $ 处于状态 $ q_{i} $ 的概率。

\end{definition}

隐马尔可夫模型由初始状态概率向量 $ \pi $ 、状态转移概率矩阵 $ A $ 和观测概率矩 阵 $ B $ 决定。 $ \pi $ 和 $ A $ 决定状态序列， $ B $ 决定观测序列。 因此， 隐马尔可夫模型 $ \lambda $ 可以用三元符号表示， 即

\begin{equation}
\lambda=(A, B, \pi)
\end{equation}
$ A, B, \pi $ 称为隐马尔可夫模型的三要素。

从定义可知， 隐马尔可夫模型作了两个基本假设:

（1）齐次马尔可夫性假设， 即假设隐藏的马尔可夫链在任意时刻 $ t $ 的状态只 依赖于其前一时刻的状态， 与其他时刻的状态及观测无关， 也与时刻 $ t $ 无关。
\begin{equation}
P\left(i_{t} \mid i_{t-1}, o_{t-1}, \cdots, i_{1}, o_{1}\right)=P\left(i_{t} \mid i_{t-1}\right), \quad t=1,2, \cdots, T
\end{equation}

（2）观测独立性假设， 即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态， 与其他观测及状态无关。
\begin{equation}
P\left(o_{t} \mid i_{T}, o_{T}, i_{T-1}, o_{T-1}, \cdots, i_{t+1}, o_{t+1}, i_{t}, i_{t-1}, o_{t-1}, \cdots, i_{1}, o_{1}\right)=P\left(o_{t} \mid i_{t}\right)
\end{equation}

\section{观测序列的生成过程}

\begin{algorithm}[htbp]
    \caption{观测序列的生成过程}
    \KwIn{隐马尔可夫模型 $ \lambda=(A, B, \pi) $, 观测序列长度 $ T $}
    \KwOut{观测序列 $ O=\left(o_{1}, o_{2}, \cdots, o_{T}\right) $}

    按照初始状态分布 $ \pi $ 产生状态 $ i_{1} $\;
    $ t:=1 $\;

    \While(){$t \le T$}{
        按照状态 $ i_{t} $ 的观测概率分布 $ b_{i}(k) $ 生成 $ o_{t} $\;
        按照状态 $ i_{t} $ 的状态转移概率分布 $ \left\{a_{i_{t} i_{t+1}}\right\} $ 产生状态 $ i_{t+1}, i_{t+1}=1,2, \cdots, N $\; 
        $t := t + 1$\;
    }
\end{algorithm}

\section{隐马尔可夫模型的3个基本问题}

\begin{enumerate}
    \item 概率计算问题。 给定模型 $ \lambda=(A, B, \pi) $ 和观测序列 $ O=\left(o_{1}, o_{2}, \cdots, o_{T}\right) $, 计 算在模型 $ \lambda $ 下观测序列 $ O $ 出现的概率 $ P(O \mid \lambda) $.
    \item 学习问题。 已知观测序列 $ O=\left(o_{1}, o_{2}, \cdots, o_{T}\right) $, 估计模型 $ \lambda=(A, B, \pi) $ 参数， 使得在该模型下观测序列概率 $ P(O \mid \lambda) $ 最大。 即用极大似然估计的方法估计参数。
    \item 预测问题， 也称为解码 (decoding）问题。 已知模型 $ \lambda=(A, B, \pi) $ 和观测 序列 $ O=\left(o_{1}, o_{2}, \cdots, o_{T}\right) $, 求对给定观测序列条件概率 $ P(I \mid O) $ 最大的状态序列$ I=\left(i_{1}, i_{2}, \cdots, i_{T}\right) $. 即给定观测序列， 求最有可能的对应的状态序列。
\end{enumerate}

\section{概率计算算法}

\subsection{直接计算法}

状态序列 $ I=\left(i_{1}, i_{2}, \cdots, i_{T}\right) $ 的概率是

\begin{equation} P(I \mid \lambda)=\pi_{i_{1}} a_{i, i_{2}} a_{i_{2} i_{3}} \cdots a_{i_{T-1} i_{T}} \end{equation}


对固定的状态序列 $ I=\left(i_{1}, i_{2}, \cdots, i_{T}\right) $, 观测序列 $ O=\left(o_{1}, o_{2}, \cdots, o_{T}\right) $ 的概率是 $ P(O \mid I, \lambda) $,

\begin{equation} P(O \mid I, \lambda)=b_{i_{1}}\left(o_{1}\right) b_{i_{2}}\left(o_{2}\right) \cdots b_{i_{T}}\left(o_{T}\right) \end{equation}

$ O $ 和 $ I $ 同时出现的联合概率为
\begin{equation} 
\begin{aligned}
    P(O, I \mid \lambda)&=P(O \mid I, \lambda) P(I \mid \lambda)\\
    &=\pi_{i_{1}} b_{i_{1}}\left(o_{1}\right) a_{i i_{2}} b_{i_{2}}\left(o_{2}\right) \cdots a_{\left.i_{i-1}\right\}} b_{i_{t}}\left(o_{T}\right)
\end{aligned}
\end{equation}

然后， 对所有可能的状态序列 $ I $ 求和， 得到观测序列 $ O $ 的概率 $ P(O \mid \lambda) $, 即
\begin{equation} 
\begin{aligned}
    P(O \mid \lambda)
&=\sum_{l} P(O \mid I, \lambda) P(I \mid \lambda) \\
&= \sum_{i_{1}, i_{2}, \cdots, i_{T}} \pi_{i_{1}} b_{i_{1}}\left(o_{1}\right) a_{i, i_{2}} b_{i_{2}}\left(o_{2}\right) \cdots a_{i_{r-1} i_{T}} b_{i_{T}}\left(o_{T}\right) 
\end{aligned}
 \end{equation}

\begin{remark}
    计算量很大， 是 $ O\left(T N^{T}\right) $ 阶的， 这种算法不可行。
\end{remark}

\subsection{前向算法}

\begin{definition}[前向概率]
    给定隐马尔可夫模型 $ \lambda $, 定义到时刻 $ t $ 部分观测序列 为 $ o_{1}, o_{2}, \cdots, o_{t} $ 且状态为 $ q_{i} $ 的概率为前向概率， 记作
\begin{equation}
\alpha_{t}(i)=P\left(o_{1}, o_{2}, \cdots, o_{t}, i_{t}=q_{i} \mid \lambda\right)
\end{equation}
\end{definition}

可以递推地求得前向概率 $ \alpha_{t}(i) $ 及观测序列概率 $ P(O \mid \lambda) $.

\begin{algorithm}[htbp]
    \caption{观测序列概率的前向算法}

    \KwIn{隐马尔可夫模型 $ \lambda $, 观测序列 $ O $}
    \KwOut{观测序列概率 $ P(O \mid \lambda) $}

    \begin{equation} \alpha_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), \quad i=1,2, \cdots, N \end{equation}\;
    对 $ t=1,2, \cdots, T-1 $,
\begin{equation}
\alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), \quad i=1,2, \cdots, N
\end{equation}\;
\begin{equation} P(O \mid \lambda)=\sum_{i=1}^{N} \alpha_{T}(i) \end{equation}\;
\end{algorithm}

如图所示， 前向算法实际是基于“状态序列的路径结构”递推计算 $ P(O \mid \lambda) $ 的算法。 前向算法高效的关键是其局部计算前向概率， 然后利用路径结构将前向 概率 “递推” 到全局， 得到 $ P(O \mid \lambda) $. 具体地， 在时刻 $ t=1 $, 计算 $ \alpha_{1}(i) $ 的 $ N $ 个值$ (i=1,2, \cdots, N) $; 在各个时刻 $ t=1,2, \cdots, T-1 $, 计算 $ \alpha_{t+1}(i) $ 的 $ N $ 个值 $ (i=1,2, \cdots, N) $, 而且每个 $ \alpha_{t+1}(i) $ 的计算利用前一时刻 $ N $ 个 $ \alpha_{t}(j) $. 减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果，避免重复计算，这样，利用前向概率计算$ P(O \mid \lambda) $ 的计算量是 $ O\left(N^{2} T\right) $ 阶的， 而不是直接计算的 $ O\left(T N^{T}\right) $ 阶。

\subsection{后向算法}

\begin{definition}[后向概率]
    给定隐马尔可夫模型 $\lambda$, 定义在时刻 $t$ 状态为 $q_{i}$ 的 条件下， 从 $t+1$ 到 $T$ 的部分观测序列为 $o_{t+1}, o_{t+2}, \cdots, o_{T}$ 的概率为后向概率， 记作
\begin{equation}
\beta_{t}(i)=P\left(o_{t+1}, o_{t+2}, \cdots, o_{T} \mid i_{t}=q_{i}, \lambda\right)
\end{equation}

\end{definition}

可以用递推的方法求得后向概率 $\beta_{t}(i)$ 及观测序列概率 $P(O \mid \lambda)$.

\begin{algorithm}[htbp]
    \caption{观测序列概率的后向算法}
    \KwIn{隐马尔可夫模型 $ \lambda $, 观测序列 $ O $}
    \KwOut{观测序列概率 $ P(O \mid \lambda) $}

    \begin{equation} \beta_{T}(i)=1, \quad i=1,2, \cdots, N \end{equation}\;

    对   $ t=T-1, T-2, \cdots, 1 $
    \begin{equation} \beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left(o_{t+1}\right) \beta_{t+1}(j), \quad i=1,2, \cdots, N \end{equation}\;

    \begin{equation} P(O \mid \lambda)=\sum_{i=1}^{N} \pi_{i} b_{i}\left(o_{1}\right) \beta_{1}(i) \end{equation}\;
\end{algorithm}

步骤 (1) 初始化后向概率， 对最终时刻的所有状态 $ q_{i} $ 规定 $ \beta_{T}(i)=1 $. 步骤 $ (2) $ 是后向概率的递推公式。 如图 $ 10.3 $ 所示， 为了计算在时刻 $ t $ 状态为 $ q_{i} $ 条件下时刻$ t+1 $ 之后的观测序列为 $ o_{t+1}, o_{t+2}, \cdots, o_{T} $ 的后向概率 $ \beta_{t}(i) $, 只需考虑在时刻 $ t+1 $ 所有可能的 $ N $ 个状态 $ q_{j} $ 的转移概率（即 $ a_{i j} $ 项), 以及在此状态下的观测 $ o_{t+1} $ 的观测概率 (即 $ b_{j}\left(o_{t+1}\right) $ 项 $ ) $, 然后考虑状态 $ q_{j} $ 之后的观测序列的后向概率(即 $ \beta_{t+1}(j) $ 项 $ ) $. 步骤（3) 求 $ P(O \mid \lambda) $ 的思路与步骤（2）一致， 只是初始概率 $ \pi_{i} $ 代替转移概率。

利用前向概率和后向概率的定义可以将观测序列概率 $ P(O \mid \lambda) $ 统一写成
\begin{equation}
P(O \mid \lambda)=\sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{t}(i) a_{i j} b_{j}\left(o_{t+1}\right) \beta_{t+1}(j), t=1,2, \cdots, T-1
\end{equation}

\subsection{概率与期望值的计算}

\begin{theorem}
    \label{Thm:ComputeGamma}
    给定模型 $ \lambda $ 和观测 $ O $, 在时刻 $ t $ 处于状态 $ q_{i} $ 的概率。 
    
    记$ \gamma_{t}(i)=P\left(i_{t}=q_{i} \mid O, \lambda\right) $

\begin{equation} \gamma_{t}(i)=\frac{\alpha_{1}(i) \beta_{t}(i)}{P(O \mid \lambda)}=\frac{\alpha_{t}(i) \beta_{t}(i)}{\sum_{j=1}^{N} \alpha_{t}(j) \beta_{t}(j)} \end{equation}
\end{theorem}


\begin{proof}
    可以通过前向后向概率计算。 事实上，
\begin{equation}
\gamma_{t}(i)=P\left(i_{t}=q_{i} \mid O, \lambda\right)=\frac{P\left(i_{t}=q_{i}, O \mid \lambda\right)}{P(O \mid \lambda)}
\end{equation}

由前向概率 $ \alpha_{t}(i) $ 和后向概率 $ \beta_{t}(i) $ 定义可知:
\begin{equation}
\alpha_{t}(i) \beta_{t}(i)=P\left(i_{t}=q_{i}, O \mid \lambda\right)
\end{equation}

所以
\begin{equation} \gamma_{t}(i)=\frac{\alpha_{t}(i) \beta_{t}(i)}{P(O \mid \lambda)}=\frac{\alpha_{t}(i) \beta_{t}(i)}{\sum_{j=1}^{N} \alpha_{t}(j) \beta_{t}(j)} \end{equation}
\end{proof}

\begin{theorem}
    \label{Thm:ComputeXi}
    给定模型 $ \lambda $ 和观测 $ O $, 在时刻 $ t $ 处于状态 $ q_{i} $ 且在时刻 $ t+1 $ 处于状态 $ q_{j} $ 的概 率。 
    
    记$ \xi_{t}(i, j)=P\left(i_{t}=q_{i}, i_{t+1}=q_{j} \mid O, \lambda\right) $

\begin{equation} \xi_{t}(i, j)=\frac{\alpha_{t}(i) a_{i j} b_{j}\left(o_{t+1}\right) \beta_{t+1}(j)}{\sum_{i=1}^{N} \sum_{i=1}^{N} \alpha_{t}(i) a_{i j} b_{j}\left(o_{t+1}\right) \beta_{t+1}(j)} \end{equation}
\end{theorem}



\begin{proof}
    可以通过前向后向概率计算:
\begin{equation}
\xi_{t}(i, j)=\frac{P\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \mid \lambda\right)}{P(O \mid \lambda)}=\frac{P\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \mid \lambda\right)}{\sum_{i=1}^{N} \sum_{j=1}^{N} P\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \mid \lambda\right)}
\end{equation}

而
\begin{equation}
P\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \mid \lambda\right)=\alpha_{t}(i) a_{i j} b_{j}\left(o_{t+1}\right) \beta_{t+1}(j)
\end{equation}


所以
\begin{equation} \xi_{t}(i, j)=\frac{\alpha_{t}(i) a_{i j} b_{j}\left(o_{t+1}\right) \beta_{t+1}(j)}{\sum_{i=1}^{N} \sum_{i=1}^{N} \alpha_{t}(i) a_{i j} b_{j}\left(o_{t+1}\right) \beta_{t+1}(j)} \end{equation}
\end{proof}

将 $ \gamma_{t}(i) $ 和 $ \xi_{i}(i, j) $ 对各个时刻 $ t $ 求和， 可以得到一些有用的期望值;

\begin{theorem}
   在观测 $ O $ 下状态 $ i $ 出现的期望值
\begin{equation}
\sum_{t=1}^{T} \gamma_{t}(i)
\end{equation} 
\end{theorem}

\begin{theorem}
    在观测 $ O $ 下由状态 $ i $ 转移的期望值
\begin{equation}
\sum_{t=1}^{T-1} \gamma_{t}(i)
\end{equation}
\end{theorem}

\begin{theorem}
    在观测 $ O $ 下由状态 $ i $ 转移到状态 $ j $ 的期望值
\begin{equation} \sum_{t=1}^{T-1} \xi_{t}(i, j) \end{equation}
\end{theorem}

\section{学习算法}

\subsection{监督学习方法}

隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还是只有观测序列，可以分别由监督学习与非监督学习实现。 

假设已给训练数据包含 $ S $ 个长度相同的观测序列和对应的状态序列 \begin{equation} \left\{\left(O_{1}, I_{1}\right),\left(O_{2}, I_{2}\right), \cdots,\left(O_{s}, I_{s}\right)\right\} \end{equation}
那么可以利用\term{极大似然估计法}来估计隐马尔可夫模型的参数。 具体方法如下。

\begin{algorithm}[htbp]
    \caption{极大似然估计法}
    
    \KwIn{包含 $ S $ 个长度相同的观测序列和对应的状态序列 $ \left\{\left(O_{1}, I_{1}\right),\left(O_{2}, I_{2}\right), \cdots,\left(O_{s}, I_{s}\right)\right\} $}

    转移概率 $ a_{i j} $ 的估计：设样本中时刻 $ t $ 处于状态 $ i $ 时刻 $ t+1 $ 转移到状态 $ j $ 的频数为 $ A_{i j} $, 那么状态转 移概率 $ a_{i j} $ 的估计是
    
    \begin{equation} \hat{a}_{i j}=\frac{A_{i j}}{\sum_{j=1}^{N} A_{i j}}, \quad i=1,2, \cdots, N ; j=1,2, \cdots, N \end{equation}
    \;
    观测概率 $ b_{j}(k) $ 的估计:设样本中状态为 $ j $ 并观测为 $ k $ 的频数是 $ B_{j k} $, 那么状态为 $ j $ 观测为 $ k $ 的概率$ b_{j}(k) $ 的估计是

    \begin{equation}
    \hat{b}_{j}(k)=\frac{B_{j k}}{\sum_{k=1}^{M} B_{j k}}, \quad j=1,2, \cdots, N ; k=1,2, \cdots, M
    \end{equation}\;
    初始状态概率 $ \pi_{i} $ 的估计 $ \hat{\pi}_{i} $ 为 $ S $ 个样本中初始状态为 $ q_{i} $ 的频率\;

\end{algorithm}

\subsection{Baum-Welch算法}

假设给定训练数据只包含 $ S $ 个长度为 $ T $ 的观测序列 $ \left\{O_{1}, O_{2}, \cdots, O_{s}\right\} $ 而没有对 应的状态序列， 目标是学习隐马尔可夫模型 $ \lambda=(A, B, \pi) $ 的参数。 我们将观测序列数据看作观测数据 $ O $, 状态序列数据看作不可观测的隐数据 $ I $, 那么隐马尔可 夫模型事实上是一个含有隐变量的概率模型
\begin{equation}
P(O \mid \lambda)=\sum_{I} P(O \mid I, \lambda) P(I \mid \lambda)
\end{equation}
它的参数学习可以由EM算法实现。 

\begin{theorem}
    将式\ref{Eqn:ComputePI}、\ref{Eqn:ComputeA}、\ref{Eqn:ComputeB}中的各概率分别用 $ \gamma_{t}(i), \xi_{t}(i, j) $ 表示， 则可将相应 的公式写成:

    \begin{equation} a_{i j}=\frac{\sum_{t=1}^{T-1} \xi_{t}(i, j)}{\sum_{t=1}^{T-1} \gamma_{t}(i)} \end{equation}

    \begin{equation} b_{j}(k)=\frac{\sum_{t=1, o_{t}=v_{k}}^{T} \gamma_{t}(j)}{\sum_{t=1}^{T} \gamma_{t}(j)} \end{equation}

    \begin{equation} \pi_{i}=\gamma_{1}(i) \end{equation}

    其中， $ \gamma_{t}(i)$由式\ref{Thm:ComputeGamma}给出， $\xi_{t}(i, j) $由式 \ref{Thm:ComputeXi} 给出
\end{theorem}


\begin{proof}
    1.确定完全数据的对数似然函数：所有观测数据写成 $ O=\left(o_{1}, o_{2}, \cdots, o_{T}\right) $, 所有隐数据写成 $ I=\left(i, i_{2}, \cdots, i_{T}\right) $, 完全 数据是 $ (O, I)=\left(o_{1}, o_{2}, \cdots, o_{T}, i_{1}, i_{2}, \cdots, i_{r}\right) . $ 完全数据的对数似然函数是 $ \log P(O, I \mid \lambda) $


EM 算法的 $ {E} $ 步: 求 $ Q $ 函数 $ Q(\lambda, \bar{\lambda}) $
\begin{equation}
Q(\lambda, \bar{\lambda})=\sum_{I} \log P(O, I \mid \lambda) P(O, I \mid \bar{\lambda})
\end{equation}

其中， $ \bar{\lambda} $ 是隐马尔可夫模型参数的当前估计值， $ \lambda $ 是要极大化的隐马尔可夫模型参数。
\begin{equation} P(O, I \mid \lambda)=\pi_{i_{1}} b_{i_{1}}\left(o_{1}\right) a_{i_1 i_{2}} b_{i_{2}}\left(o_{2}\right) \cdots a_{i_{T-1} i_{T}} b_{i_{r}}\left(o_{T}\right) \end{equation}

于是函数 $ Q(\lambda, \bar{\lambda}) $ 可以写成:

\begin{equation}
    \label{Eqn:ComputeQ}
\begin{aligned}
Q(\lambda, \bar{\lambda})=& \sum_{I} \log \pi_{i_1} P(O, I \mid \bar{\lambda}) \\
&+\sum_{I}\left(\sum_{t=1}^{T-1} \log a_{i_t i_{t+1}}\right) P(O, I \mid \bar{\lambda})+\sum_{I}\left(\sum_{t=1}^{T} \log b_{i_{t}}\left(o_{t}\right)\right) P(O, I \mid \bar{\lambda})
\end{aligned}
\end{equation}


式中求和都是对所有训练数据的序列总长度 $ T $ 进行的。


3. EM 算法的 $ {M} $ 步: 极大化 $ Q $ 函数 $ Q(\lambda, \bar{\lambda}) $ 求模型参数 $ A, B, \pi $ 由于要极大化的参数在式 中单独地出现在 3 个项中， 所以只需对各 项分别极大化。

(1) 式 \ref{Eqn:ComputeQ} 的第 1 项可以写成;
\begin{equation}
\sum_{I} \log \pi_{i_{0}} P(O, I \mid \bar{\lambda})=\sum_{i=1}^{N} \log \pi_{i} P\left(O, i_{1}=i \mid \bar{\lambda}\right)
\end{equation}

注意到 $ \pi_{i} $ 满足约束条件 $ \sum_{i=1}^{N} \pi_{i}=1 $, 利用拉格朗日乘子法， 写出拉格朗日函数:
\begin{equation}
\sum_{i=1}^{N} \log \pi_{i} P\left(O, i_{1}=i \mid \bar{\lambda}\right)+\gamma\left(\sum_{i=1}^{N} \pi_{i}-1\right)
\end{equation}

对其求偏导数并令结果为 0


\begin{equation}
    \label{Eqn:PartialDerivativeOfLagrange}
    \frac{\partial}{\partial \pi_{i}}\left[\sum_{i=1}^{N} \log \pi_{i} P\left(O, i_{1}=i \mid \bar{\lambda}\right)+\gamma\left(\sum_{i=1}^{N} \pi_{i}-1\right)\right]=0
\end{equation}


得
\begin{equation}
P\left(O, i_{1}=i \mid \bar{\lambda}\right)+\gamma \pi_{i}=0
\end{equation}
对 $ i $ 求和得到 $ \gamma $
\begin{equation}
\gamma=-P(O \mid \bar{\lambda})
\end{equation}

代入式 \ref{Eqn:PartialDerivativeOfLagrange}  即得

\begin{equation}
    \label{Eqn:ComputePI}
\pi_{i}=\frac{P\left(O, i_{1}=i \mid \bar{\lambda}\right)}{P(O \mid \bar{\lambda})}
\end{equation}



(2) 式 \ref{Eqn:ComputeQ} 的第 2 项可以写成
\begin{equation}
\sum_{I}\left(\sum_{t=1}^{T-1} \log a_{i_t i_{i+1}}\right) P(O, I \mid \bar{\lambda})=\sum_{i=1}^{N} \sum_{j=1}^{N} \sum_{t=1}^{T-1} \log a_{i j} P\left(O, i_{t}=i, i_{t+1}=j \mid \bar{\lambda}\right)
\end{equation}

类似第 1 项， 应用具有约束条件 $ \sum_{j=1}^{N} a_{i j}=1 $ 的拉格朗日乘子法可以求出

\begin{equation}
    \label{Eqn:ComputeA}
a_{i j}=\frac{\sum_{t=1}^{T-1} P\left(O, i_{t}=i, i_{t+1}=j \mid \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left(O, i_{t}=i \mid \bar{\lambda}\right)} 
\end{equation}




(3)式 \ref{Eqn:ComputeQ} 的第 3 项为
\begin{equation}
\sum_{I}\left(\sum_{t=1}^{T} \log b_{i_{t}}\left(o_{t}\right)\right) P(O, I \mid \bar{\lambda})=\sum_{j=1}^{N} \sum_{t=1}^{T} \log b_{j}\left(o_{t}\right) P\left(O, i_{t}=j \mid \bar{\lambda}\right)
\end{equation}

同样用拉格朗日乘子法， 约束条件是 $ \sum_{k=1}^{M} b_{j}(k)=1 $. 注意， 只有在 $ o_{t}=v_{k} $ 时 $ b_{j}\left(o_{t}\right) $ 对 $ b_{j}(k) $ 的偏导数才不为 0 , 以 $ I\left(o_{t}=v_{k}\right) $ 表示。 求得

\begin{equation}
    \label{Eqn:ComputeB}
 b_{j}(k)=\frac{\sum_{t=1}^{T} P\left(O, i_{t}=j \mid \bar{\lambda}\right) I\left(o_{t}=v_{k}\right)}{\sum_{t=1}^{T} P\left(O, i_{t}=j \mid \bar{\lambda}\right)}
\end{equation}
 

\end{proof}


\begin{algorithm}[htbp]
    \caption{Baum-Welch算法}

    \KwIn{观测数据 $ O=\left(o_{1}, o_{2}, \cdots, o_{T}\right) $}
    \KwOut{隐马尔可夫模型 $ \lambda^{(n+1)}=\left(A^{(n+1)}, B^{(n+1)}, \pi^{(n+1)}\right) $}

    初始化：
对 $ n=0 $, 选取 $ a_{i j}^{(0)}, b_{j}(k)^{(0)}, \pi_{i}^{(0)} $, 得到模型 $ \lambda^{(0)}=\left(A^{(0)}, B^{(0)}, \pi^{(0)}\right) $\;

递推。 对 $ n=1,2, \cdots $,
\begin{equation}
a_{i j}^{(n+1)}=\frac{\sum_{t=1}^{T-1} \xi_{t}(i, j)}{\sum_{t=1}^{T-1} \gamma_{t}(i)}
\end{equation}
\begin{equation} b_{j}(k)^{(n+1)}=\frac{\sum_{t=1, o_{i}=v_{k}}^{T} \gamma_{t}(j)}{\sum_{t=1}^{T} \gamma_{t}(j)} \end{equation}

\begin{equation} \pi_{i}^{(n+1)}=\gamma_{1}(i) \end{equation}
右端各值按观测 $ O=\left(o_{1}, o_{2}, \cdots, o_{T}\right) $ 和模型 $ \lambda^{(n)}=\left(A^{(n)}, B^{(n)}, \pi^{(n)}\right) $ 计算。 式中 $ \gamma_{t}(i) $, $ \xi_{t}(i, j) $ 由式 \ref{Thm:ComputeGamma} 和式 \ref{Thm:ComputeXi}  给出\;
终止。 得到模型参数 $ \lambda^{(n+1)}=\left(A^{(n+1)}, B^{(n+1)}, \pi^{(n+1)}\right) $
\end{algorithm}

\section{预测算法}

\subsection{近似算法}

近似算法的想法是， 在每个时刻 $ t $ 选择在该时刻最有可能出现的状态 $ i_{t}^{*} $, 从 而得到一个状态序列 $ I^{*}=\left(i_{1}^{*}, i_{2}^{*}, \cdots, i_{T}^{*}\right) $, 将它作为预测的结果。

给定隐马尔可夫模型 $ \lambda $ 和观测序列 $ O $, 在时刻 $ t $ 处于状态 $ q_{i} $ 的概率 $ \gamma_{t}(i) $ 是
\begin{equation}
\gamma_{t}(i)=\frac{\alpha_{t}(i) \beta_{t}(i)}{P(O \mid \lambda)}=\frac{\alpha_{t}(i) \beta_{t}(i)}{\sum_{j=1}^{N} \alpha_{t}(j) \beta_{t}(j)}
\end{equation}

在每一时刻 $ t $ 最有可能的状态 $ i_{t}^{*} $ 是
\begin{equation}
i_{t}^{*}=\arg \max _{1 \leqslant i \leqslant N}\left[\gamma_{t}(i)\right], \quad t=1,2, \cdots, T
\end{equation}

从而得到状态序列 $ I^{*}=\left(i_{1}^{*}, i_{2}^{*}, \cdots, i_{T}^{*}\right) $.

\begin{remark}
   近似算法的优点是计算简单，其缺点是不能保证预测的状态序列整体是最有
可能的状态序列，因为预测的状态序列可能有实际不发生的部分。方法得到的状态序列中有可能存在转移概率为 0 的相邻状态， 即对某些 $ i, j $, $ a_{i j}=0 $ 时。 尽管如此， 近似算法仍然是有用的。 
\end{remark}

\subsection{维特比算法}

根据动态规划的性质。 

首先导入两个变量 $ \delta $ 和 $ \psi $. 

\begin{definition}[在时刻 $ t $ 状态为 $ i $ 的所有单个路径 $ \left(i_{1}, i_{2}, \cdots, i_{t}\right) $ 中概率最大值]
    定义在时刻 $ t $ 状态为 $ i $ 的所有单个路径 $ \left(i_{1}, i_{2}, \cdots, i_{t}\right) $ 中概率最大值为
\begin{equation}
\delta_{t}(i)=\max _{i_1, i_{2}, \cdots, i_{t-1}} P\left(i_{t}=i, i_{t-1}, \cdots, i_{1}, o_{t}, \cdots, o_{1} \mid \lambda\right), \quad i=1,2, \cdots, N
\end{equation}
\end{definition}

\begin{corollary}
   由定义可得变量 $ \delta $ 的递推公式:
\begin{equation}
\begin{aligned}
\delta_{t+1}(i) &=\max _{i_1, i_2, \cdots, i_{t}} P\left(i_{t+1}=i, i_{t}, \cdots, i_{1}, o_{t+1}, \cdots, o_{1} \mid \lambda\right) \\
&=\max _{1 \leqslant j \leqslant N}\left[\delta_{t}(j) a_{j i}\right] b_{i}\left(o_{t+1}\right), \quad i=1,2, \cdots, N ; t=1,2, \cdots, T-1
\end{aligned}
\end{equation} 
\end{corollary}

\begin{definition}[在时刻 $ t $ 状态为 $ i $ 的所有单个路径 $ \left(i_{1}, i_{2}, \cdots, i_{t-1}, i\right) $ 中概率最大的路径的第 $ t-1 $ 个结点]
    定义在时刻 $ t $ 状态为 $ i $ 的所有单个路径 $ \left(i_{1}, i_{2}, \cdots, i_{t-1}, i\right) $ 中概率最大的路径的第 $ t-1 $ 个结点为
\begin{equation}
\psi_{t}(i)=\arg \max _{1 \leqslant j \leqslant N}\left[\delta_{t-1}(j) a_{j i}\right], \quad i=1,2, \cdots, N
\end{equation}
\end{definition}



\begin{algorithm}
\caption{维特比算法}

    \KwIn{模型 $ \lambda=(A, B, \pi) $ 和观测 $ O=\left(o_{1}, o_{2}, \cdots, o_{T}\right) $; 输出: 最优路径 $ I^{*}=\left(i_{1}^{*}, i_{2}^{*}, \cdots, i_{T}^{*}\right) $}
    \KwOut{最优路径 $ I^{*}=\left(i_{1}^{*}, i_{2}^{*}, \cdots, i_{T}^{*}\right) $}

    初始化
\begin{equation}
\begin{array}{cc}
\delta_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), & i=1,2, \cdots, N \\
\psi_{1}(i)=0, & i=1,2, \cdots, N
\end{array}
\end{equation}\;
递推。 对 $ t=2,3, \cdots, T $
\begin{equation}
\begin{array}{ll}
\delta_{t}(i)=\max _{1 \leqslant j \leqslant N}\left[\delta_{t-1}(j) a_{j i}\right] b_{i}\left(o_{t}\right), & i=1,2, \cdots, N \\
\psi_{t}(i)=\arg \max _{1 \leqslant j \leqslant N}\left[\delta_{t-1}(j) a_{j i}\right], & i=1,2, \cdots, N
\end{array}
\end{equation}\;
终止
\begin{equation}
\begin{array}{c}
P^{*}=\max _{1 \leqslant i \leqslant N} \delta_{T}(i) \\
i_{T}^{*}=\arg \max _{1 \leqslant i \leqslant N}\left[\delta_{T}(i)\right]
\end{array}
\end{equation}\;
最优路径回溯。 对 $ t=T-1, T-2, \cdots, 1 $
\begin{equation}
i_{t}^{*}=\psi_{t+1}\left(i_{t+1}^{*}\right)
\end{equation}
求得最优路径 $ I^{*}=\left(i_{1}^{*}, i_{2}^{*}, \cdots, i_{T}^{*}\right) $
\;

\end{algorithm}{}