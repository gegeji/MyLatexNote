\chapter{Least squares data fitting}

\section{Model Fitting}

\begin{problem}
  Suppose $ x $ and a scalar quantity $ y $ are related as

\begin{equation}
y \approx f(x)
\end{equation}

$ x $ is the \term{explanatory variable} or \term{independent variable}, $ y $ is the \term{outcome}, or \term{response variable}, or \term{dependent variable}.

We don't know $ f $, but have some idea about its general form.

\end{problem}

\begin{definition}[Model Fitting]
    Find an approximate model $ \hat{f} $ for $ f $, based on observations

    We use the notation $ \hat{y} $ for the model prediction of the outcome $ y $

    \begin{equation}
    \hat{y}=\hat{f}(x)
    \end{equation}
\end{definition}


\subsection{Prediction Error}

We have data consisting of $ N $ examples (samples, measurements, observations)

\begin{equation}
x^{(1)}, \ldots, x^{(N)}, \quad y^{(1)}, \ldots, y^{(N)}
\end{equation}

model prediction for example $ i $ is $ \hat{y}^{(i)}=\hat{f}\left(x^{(i)}\right) $.

\begin{definition}[Prediction Error, Residual]
    The \term{prediction error} or \term{residual} for example $ i $ is
\begin{equation}
r^{(i)}=y^{(i)}-\hat{y}^{(i)}=y^{(i)}-\hat{f}\left(x^{(i)}\right)
\end{equation}
\end{definition}



The model $ \hat{f} $ fits the data well if the $ N $ residuals $ r^{(i)} $ are small.

Prediction error can be quantified using the mean square error (MSE).

\begin{definition}[Mean Square Error (MSE)]
    \begin{equation}
\frac{1}{N} \sum_{i=1}^{N}\left(r^{(i)}\right)^{2}
\end{equation}
\end{definition}

The square root of the MSE is the \term{RMS error}.

\section{Least Square Regression}

We first consider the regression model

\begin{problem}
    \begin{equation}
    \hat{f}(x)=x^{T} \beta+v
    \end{equation} 

    Here, the independent variable $ x $ is an $ n $-vector, the elements of $ x $ are the \term{regressors}.
    
\end{problem}

The model is parameterized by the weight vector $ \beta $ and the offset (intercept) $ v $.

\begin{theorem}
    The prediction error for example $ i $ is
\begin{equation}
\begin{aligned}
r^{(i)} &=y^{(i)}-\hat{f}\left(x^{(i)}\right) \\
&=y^{(i)}-\left(x^{(i)}\right)^{T} \beta-v
\end{aligned}
\end{equation}
\end{theorem}

\begin{theorem}
    The MSE is
\begin{equation}
\frac{1}{N} \sum_{i=1}^{N}\left(r^{(i)}\right)^{2}=\frac{1}{N} \sum_{i=1}^{N}\left(y^{(i)}-\left(x^{(i)}\right)^{T} \beta-v\right)^{2}
\end{equation}
\end{theorem}




\begin{problem}
    Choose the model parameters $ v, \beta $ that minimize the MSE
    \begin{equation}
    \frac{1}{N} \sum_{i=1}^{N}\left(v+\left(x^{(i)}\right)^{T} \beta-y^{(i)}\right)^{2}
    \end{equation}

\end{problem}


This is a least squares problem: minimize $ \left\|A \theta-y^{\mathrm{d}}\right\|^{2} $ with
\begin{equation}
A=\left[\begin{array}{cc}
1 & \left(x^{(1)}\right)^{T} \\
1 & \left(x^{(2)}\right)^{T} \\
\vdots & \vdots \\
1 & \left(x^{(N)}\right)^{T}
\end{array}\right], \quad \theta=\left[\begin{array}{c}
v \\
\beta
\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(N)}
\end{array}\right]
\end{equation}

\begin{notation}
    We write the solution as $ \hat{\theta}=(\hat{v}, \hat{\beta}) $.
\end{notation}



\section{Linear-in-parameters Model}

\begin{problem}
    We choose the model $ \hat{f}(x) $ from a family of models
\begin{equation}
\hat{f}(x)=\theta_{1} f_{1}(x)+\theta_{2} f_{2}(x)+\cdots+\theta_{p} f_{p}(x)
\end{equation}

The functions $ f_{i} $ are scalar valued basis functions (chosen by us), the basis functions often include a constant function (typically, $ f_{1}(x)=1 $ ). 

The coefficients $ \theta_{1}, \ldots, \theta_{p} $ are the model parameters.
\end{problem}

The model $ \hat{f}(x) $ is linear in the parameters $ \theta_{i} $.

\begin{corollary}
    If $ f_{1}(x)=1 $, this can be interpreted as a regression model

\begin{equation}
\hat{y}=\beta^{T} \tilde{x}+v
\end{equation}

with parameters $ v=\theta_{1}, \beta=\theta_{2: p} $ and new features $ \tilde{x} $ generated from $ x $ :
\begin{equation}
\tilde{x}_{1}=f_{2}(x), \quad \ldots, \quad \tilde{x}_{p}=f_{p}(x)
\end{equation}
\end{corollary}




\section{Least Squares Model Fitting}

\begin{problem}
    Fit linear-in-parameters model to data set $ \left(x^{(1)}, y^{(1)}\right), \ldots,\left(x^{(N)}, y^{(N)}\right) $.
\end{problem}

\begin{theorem}
    Residual for data sample $ i $ is
\begin{equation}
r^{(i)}=y^{(i)}-\hat{f}\left(x^{(i)}\right)=y^{(i)}-\theta_{1} f_{1}\left(x^{(i)}\right)-\cdots-\theta_{p} f_{p}\left(x^{(i)}\right)
\end{equation}
\end{theorem}

\begin{problem}[Least Squares Model Fitting]
    Choose parameters $ \theta $ by minimizing MSE

    \begin{equation}
    \frac{1}{N}\left(\left(r^{(1)}\right)^{2}+\left(r^{(2)}\right)^{2}+\cdots+\left(r^{(N)}\right)^{2}\right)
    \end{equation}
\end{problem}

this is a least squares problem: 

\begin{problem}
    minimize $ \left\|A \theta-y^{\mathrm{d}}\right\|^{2} $ with

\begin{equation}
A=\left[\begin{array}{ccc}
f_{1}\left(x^{(1)}\right) & \cdots & f_{p}\left(x^{(1)}\right) \\
f_{1}\left(x^{(2)}\right) & \cdots & f_{p}\left(x^{(2)}\right) \\
\vdots & & \vdots \\
f_{1}\left(x^{(N)}\right) & \cdots & f_{p}\left(x^{(N)}\right)
\end{array}\right], \quad \theta=\left[\begin{array}{c}
\theta_{1} \\
\theta_{2} \\
\vdots \\
\theta_{p}
\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(N)}
\end{array}\right]
\end{equation}
\end{problem}



\subsection{Example: Polynomial Approximation}

\begin{problem}
    \begin{equation}
\hat{f}(x)=\theta_{1}+\theta_{2} x+\theta_{3} x^{2}+\cdots+\theta_{p} x^{p-1}
\end{equation}
\end{problem}

This is a linear-in-parameters model with basis functions $ 1, x, \ldots, x^{p-1} $.

\begin{problem}[least squares model fitting]
    choose parameters $ \theta $ by minimizing MSE
\begin{equation}
\frac{1}{N}\left(\left(y^{(1)}-\hat{f}\left(x^{(1)}\right)\right)^{2}+\left(y^{(2)}-\hat{f}\left(x^{(2)}\right)\right)^{2}+\cdots+\left(y^{(N)}-\hat{f}\left(x^{(N)}\right)\right)^{2}\right)
\end{equation}
\end{problem}

\begin{problem}[least squares model fitting in matrix notation] 
    minimize $ \left\|A \theta-y^{\mathrm{d}}\right\|^{2} $ with
    \begin{equation}
    A=\left[\begin{array}{ccccc}
    1 & x^{(1)} & \left(x^{(1)}\right)^{2} & \cdots & \left(x^{(1)}\right)^{p-1} \\
    1 & x^{(2)} & \left(x^{(2)}\right)^{2} & \cdots & \left(x^{(2)}\right)^{p-1} \\
    \vdots & \vdots & \vdots & & \vdots \\
    1 & x^{(N)} & \left(x^{(N)}\right)^{2} & \cdots & \left(x^{(N)}\right)^{p-1}
    \end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}
    y^{(1)} \\
    y^{(2)} \\
    \vdots \\
    y^{(N)}
    \end{array}\right]
    \end{equation}
\end{problem}


\section{Piecewise-affine Function}

\begin{definition}[Knot Points]
    $ a_{1}<a_{2}<\cdots<a_{k} $ on the real axis.
\end{definition}

\begin{proposition}
    Piecewise-affine function is continuous, and affine on each interval $ \left[a_{k}, a_{k+1}\right] $.
\end{proposition}

\begin{theorem}
    Piecewise-affine function with knot points $ a_{1}, \ldots, a_{k} $ can be written as

    \begin{equation}
    \hat{f}(x)=\theta_{1}+\theta_{2} x+\theta_{3}\left(x-a_{1}\right)_{+}+\cdots+\theta_{2+k}\left(x-a_{k}\right)_{+}
    \end{equation}

    where $ u_{+}=\max \{u, 0\} $.
\end{theorem}



\subsection{Piecewise-Affine Function Fitting}

Piecewise-affine model is in linear in the parameters $ \theta $, with basis functions

\begin{equation}
f_{1}(x)=1, \quad f_{2}(x)=x, \quad f_{3}(x)=\left(x-a_{1}\right)_{+}, \quad \ldots, \quad f_{k+2}(x)=\left(x-a_{k}\right)_{+}
\end{equation}

\begin{example}
    Fit piecewise-affine function with knots $ a_{1}=-1, a_{2}=1 $ to 100 points.
\end{example}

\section{Time Series Trend}

\begin{definition}[Trend Line]
    Suppose $ N $ data samples from time series: $ y^{(i)} $ is value at time $ i $, for $ i=1, \ldots, N $.

    straight-line fit $ \hat{y}^{(i)}=\theta_{1}+\theta_{2} i $ is the \term{trend line}.
\end{definition}

\begin{definition}[de-trended time series]
    $ y^{\mathrm{d}}-\hat{y}^{\mathrm{d}}=\left(y^{(1)}-\hat{y}^{(1)}, \ldots, y^{(N)}-\hat{y}^{(N)}\right) $ is the de-trended time series
\end{definition}

\begin{problem}
    Least squares fitting of trend line: minimize $ \left\|A \theta-y^{\mathrm{d}}\right\|^{2} $ with

\begin{equation} A=\left[\begin{array}{cc}1 & 1 \\ 1 & 2 \\ 1 & 3 \\ \vdots & \vdots \\ 1 & N\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}y^{(1)} \\ y^{(2)} \\ y^{(3)} \\ \vdots \\ y^{(N)}\end{array}\right] \end{equation}
\end{problem}


\subsection{Example: World Petroleum Consumption}


Time series of world petroleum consumption (million barrels/day) versus year

Left figure shows data samples and trend line

Right figure shows de-trended time series

% todo (2021-12-18 22:10): figure

\subsection{Trend Plus Seasonal Component}

\begin{proposition}
    Model time series can be decomposed as a linear trend plus a periodic component with period $ P $

    \begin{equation}
    \hat{y}^{\mathrm{d}}=\hat{y}^{\text {lin }}+\hat{y}^{\text {seas }}
    \end{equation}

    with \begin{equation} \hat{y}^{\operatorname{lin}}=\theta_{1}(1,2, \ldots, N) \end{equation} and
\begin{equation}
\hat{y}^{\text {seas }}=\left(\theta_{2}, \theta_{3}, \ldots, \theta_{P+1}, \theta_{2}, \theta_{3}, \ldots, \theta_{P+1}, \ldots, \theta_{2}, \theta_{3}, \ldots, \theta_{P+1}\right)
\end{equation}
\end{proposition}

\begin{definition}[Offset]
    the mean of $ \hat{y}^{\text {seas }} $ serves as a \term{constant offset}.
\end{definition}

\begin{definition}[De-Trended, Seasonally Adjusted Time Series]
    Residual $ y^{\mathrm{d}}-\hat{y}^{\mathrm{d}} $ is the de-trended, seasonally adjusted time series.
\end{definition}

\begin{problem}
    Least squares formulation: minimize $ \left\|A \theta-y^{\mathrm{d}}\right\|^{2} $ with

\begin{equation} A_{1: N, 1}=\left[\begin{array}{c}1 \\ 2 \\ \vdots \\ N\end{array}\right], \quad A_{1: N, 2: P+1}=\left[\begin{array}{c}I_{P} \\ I_{P} \\ \vdots \\ I_{P}\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(N)}\end{array}\right] \end{equation}
\end{problem}



\section{Auto-Regressive (AR) Time Series Model}

\begin{problem}
    \begin{equation}
\hat{z}_{t+1}=\beta_{1} z_{t}+\cdots+\beta_{M} z_{t-M+1}, \quad t=M, M+1, \ldots
\end{equation}

$ z_{1}, z_{2}, \ldots $ is a time series, $ \hat{z}_{t+1} $ is a prediction of $ z_{t+1} $, made at time $ t $. Prediction $ \hat{z}_{t+1} $ is a linear function of previous $ M $ values $ z_{t}, \ldots, z_{t-M+1} $. $ M $ is the memory of the model.
\end{problem}

\begin{problem}[Least squares fitting of AR model]
    
    Given oberved data $ z_{1}, \ldots, z_{T} $, minimize
\begin{equation}
\left(z_{M+1}-\hat{z}_{M+1}\right)^{2}+\left(z_{M+2}-\hat{z}_{M+2}\right)^{2}+\cdots+\left(z_{T}-\hat{z}_{T}\right)^{2}
\end{equation}
\end{problem}


\begin{problem}[Least square formulation for AR model]
    This is a least squares problem: 
    
    minimize $ \left\|A \beta-y^{\mathrm{d}}\right\|^{2} $ with
\begin{equation}
A=\left[\begin{array}{cccc}
z_{M} & z_{M-1} & \cdots & z_{1} \\
z_{M+1} & z_{M} & \cdots & z_{2} \\
\vdots & \vdots & & \vdots \\
z_{T-1} & z_{T-2} & \cdots & z_{T-M}
\end{array}\right], \quad \beta=\left[\begin{array}{c}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{M}
\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}
z_{M+1} \\
z_{M+2} \\
\vdots \\
z_{T}
\end{array}\right]
\end{equation}

\end{problem}



\section{Generalization and Validation}

\begin{definition}[Generalization Ability]
    ability of model to predict outcomes for new, unseen data
\end{definition}

\begin{definition}[Model Validation (Out-of-sample Validation)]
    To assess generalization ability, divide data in two sets: training set and test (or validation) set. Use training set to fit model and use test set to get an idea of generalization ability
\end{definition}

\begin{definition}[Over-fit Model]
    model with low prediction error on training set, bad generalization ability. Prediction error on training set is much smaller than on test set.
\end{definition}


\subsection{Cross-validation}

It is an extension of out-of-sample validation.

\begin{algorithm}[htbp]
    \caption{Cross-validation}
    divide data in $ K $ sets (folds); typical values are $ K=5, K=10 $\;
    \For(){$ i=1 $ to $ K $}{
        fit model $ i $ using fold $ i $ as test set and other data as training set\;
        compare parameters and train/test RMS errors for the $ K $ models
    }
\end{algorithm}


\section{Boolean (two-way) Classification}

\begin{problem}
    A data fitting problem where the outcome $ y $ can take two values $ +1,-1 $.

    Values of $ y $ represent two categories (true/false, spam/not spam, ...). Model $ \hat{y}=\hat{f}(x) $ is called a \term{Boolean classifier}.

\end{problem}

Use least squares to fit model $ \tilde{f}(x) $ to training set $ \left(x^{(1)}, y^{(1)}\right), \ldots,\left(x^{(N)}, y^{(N)}\right) $.

$ \tilde{f}(x) $ can be a regression model $ \tilde{f}(x)=x^{T} \beta+v $ or linear in parameters
\begin{equation}
\tilde{f}(x)=\theta_{1} f_{1}(x)+\cdots+\theta_{p} f_{p}(x)
\end{equation}

Take sign of $ \tilde{f}(x) $ to get a Boolean classifier
\begin{equation}
\hat{f}(x)=\operatorname{sign}(\tilde{f}(x))=\left\{\begin{array}{ll}
+1 & \text { if } \tilde{f}(x) \geq 0 \\
-1 & \text { if } \tilde{f}(x)<0
\end{array}\right.
\end{equation}

\subsection{Example: Handwritten Digit Classification}

\begin{remark}
    Illustrations of this example are needed to better understand this example.
\end{remark}

\begin{itemize}
    \item $ 28 \times 28 $ images of handwritten digits $ \left(n=28^{2}=784\right) $ pixels
    \item data set contains 60000 training examples; 10000 test examples
    \item Boolean classifier distinguishes digit zero $ (y=1) $ from other digits $ (y=-1) $

    
\end{itemize}


\subsubsection{Classifier with Basic Regression Model}

\begin{equation}
\hat{f}(x)=\operatorname{sign}(\tilde{f}(x))=\operatorname{sign}\left(x^{T} \beta+v\right)
\end{equation}

$ x $ is vector of 493 pixel intensities

figure shows distribution of $ \tilde{f}\left(x^{(i)}\right)=\left(x^{(i)}\right)^{T} \hat{\beta}+\hat{v} $ on training set.

blue bars to the left of dashed line are false negatives (misclassified digits zero)

red bars to the right of dashed line are false positives (misclassified non-zeros)

\subsubsection{Classifier with Additional Nonlinear Features}

\begin{equation}
\hat{f}(x)=\operatorname{sign}(\tilde{f}(x))=\operatorname{sign}\left(\sum_{i=1}^{p} \theta_{i} f_{i}(x)\right)
\end{equation}

basis functions include constant, 493 elements of $ x $, plus 5000 functions
\begin{equation}
f_{i}(x)=\max \left\{0, r_{i}^{T} x+s_{i}\right\} \quad \text { with randomly generated } r_{i}, s_{i}
\end{equation}

figure shows distribution of $ \tilde{f}\left(x^{(i)}\right) $ on training set

\section{Multi-class Classification}

\begin{problem}
    a data fitting problem where the outcome $ y $ can takes values $ 1, \ldots, K $

    values of $ y $ represent $ K $ labels or categories.

    multi-class classifier $ \hat{y}=\hat{f}(x) $ maps $ x $ to an element of $ \{1,2, \ldots, K\} $
\end{problem}

\subsection{Least Squares Multi-Class Classifier}

\begin{algorithm}[htbp]
    \caption{Least Squares Multi-Class Classifier}
    \For(){$ k=1, \ldots, K $}{
        compute Boolean classifier to distinguish class $ k $ from not $ k $
\begin{equation}
\hat{f}_{k}(x)=\operatorname{sign}\left(\tilde{f}_{k}(x)\right)
\end{equation}
    }
\end{algorithm}

Define multi-class classifier as

\begin{definition}[Multi-Class Classifier]
    \begin{equation}
\hat{f}(x)=\arg \underset{k=1, \ldots, K}{\max} \tilde{f}_{k}(x)
\end{equation}
\end{definition}



\section{Statistics Interpretation For Least Squares}

\begin{problem}
    \begin{equation}
y=X \beta+\epsilon
\end{equation}

\begin{itemize}
    \item $ \beta $ is (non-random) $ p $-vector of unknown parameters
    \item $ X $ is $ n \times p $ (data matrix or design matrix, i.e., result of experiment design)
    \item If there is an offset $ v $, we include it in $ \beta $ and add a column of ones in $ X $
    \item $ \epsilon $ is a random $ n $-vector (random error or disturbance)
    \item $ y $ is an observable random $ n $-vector
\end{itemize}
\end{problem}

\begin{remark}
    This notation differs from previous sections but is common in statistics.
\end{remark}

We discuss methods for estimating parameters $ \beta $ from observations of $ y $.

\subsection{Assumptions}

\begin{proposition}
    $ X $ is tall $ (n>p) $ with linearly independent columns.
\end{proposition}

\begin{proposition}
    Random disturbances $ \epsilon_{i} $ have zero mean.
\begin{equation}
\mathbf{E} \epsilon_{i}=0 \quad \text { for } i=1, \ldots, n
\end{equation}
\end{proposition}

\begin{proposition}
    Random disturbances have equal variances $ \sigma^{2} $.
\begin{equation}
\mathbf{E} \epsilon_{i}^{2}=\sigma^{2} \quad \text { for } i=1, \ldots, n
\end{equation}
\end{proposition}

\begin{proposition}
    Random disturbances are uncorrelated (have zero covariances).
\begin{equation}
\mathbf{E}\left(\epsilon_{i} \epsilon_{j}\right)=0 \quad \text { for } i, j=1, \ldots, n \text { and } i \neq j
\end{equation}
\end{proposition}

\begin{theorem}
    Last three assumptions can be combined using matrix and vector notation
\begin{equation}
\mathbf{E} \epsilon=0, \quad \mathbf{E} \epsilon \epsilon^{T}=\sigma^{2} I
\end{equation}
\end{theorem}


\subsection{Least Squares Estimator}

\begin{theorem}
    Least squares estimate $ \hat{\beta} $ of parameters $ \beta $, given the observations $ y $, is
\begin{equation}
\hat{\beta}=X^{\dagger} y=\left(X^{T} X\right)^{-1} X^{T} y
\end{equation}
\end{theorem}

\begin{FigureCenter}{Least Squares Estimator}
    

\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300

%Shape: Parallelogram [id:dp7663934145481166] 
\draw  [color={rgb, 255:red, 255; green, 255; blue, 255 }  ,draw opacity=1 ][fill={rgb, 255:red, 179; green, 179; blue, 179 }  ,fill opacity=1 ] (208.28,120.88) -- (547.27,120.88) -- (401.99,264.95) -- (63,264.95) -- cycle ;
%Straight Lines [id:da6162612682019719] 
\draw [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]  [dash pattern={on 1.69pt off 2.76pt}]  (407,169.25) -- (258.2,189.88) ;
%Straight Lines [id:da12525576068214828] 
\draw [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]    (258.2,189.88) -- (402.05,85.37) ;
\draw [shift={(406.1,82.43)}, rotate = 144] [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (14.29,-6.86) -- (0,0) -- (14.29,6.86) -- cycle    ;
%Straight Lines [id:da40949116136421404] 
\draw [color={rgb, 255:red, 234; green, 81; blue, 100 }  ,draw opacity=1 ][line width=2.25]    (407,169.25) -- (406.15,87.43) ;
\draw [shift={(406.1,82.43)}, rotate = 89.41] [fill={rgb, 255:red, 234; green, 81; blue, 100 }  ,fill opacity=1 ][line width=0.08]  [draw opacity=0] (14.29,-6.86) -- (0,0) -- (14.29,6.86) -- cycle    ;

% Text Node
\draw (103.86,209.99) node [anchor=north west][inner sep=0.75pt]  [xscale=0.75,yscale=0.75]  {$\begin{aligned}
C( X)\\
\operatorname{range}( X)
\end{aligned}$};
% Text Node
\draw (325.34,113.4) node [anchor=north west][inner sep=0.75pt]  [xscale=0.75,yscale=0.75]  {$\epsilon $};
% Text Node
\draw  [color={rgb, 255:red, 0; green, 0; blue, 0 }  ,draw opacity=0 ][fill={rgb, 255:red, 234; green, 81; blue, 100 }  ,fill opacity=0.84 ]  (421.7,113.38) .. controls (421.7,110.62) and (423.94,108.38) .. (426.7,108.38) -- (613.7,108.38) .. controls (616.46,108.38) and (618.7,110.62) .. (618.7,113.38) -- (618.7,139.38) .. controls (618.7,142.14) and (616.46,144.38) .. (613.7,144.38) -- (426.7,144.38) .. controls (423.94,144.38) and (421.7,142.14) .. (421.7,139.38) -- cycle  ;
\draw (424.7,112.78) node [anchor=north west][inner sep=0.75pt]  [xscale=0.75,yscale=0.75]  {$\hat{\beta } =X^{\dagger } y=\left( X^{T} X\right)^{-1} X^{T} y$};
% Text Node
\draw (384,43.4) node [anchor=north west][inner sep=0.75pt]  [xscale=0.75,yscale=0.75]  {$y=X\beta +\epsilon $};
% Text Node
\draw (223,185.4) node [anchor=north west][inner sep=0.75pt]  [xscale=0.75,yscale=0.75]  {$X\beta $};
% Text Node
\draw (408,160.4) node [anchor=north west][inner sep=0.75pt]  [xscale=0.75,yscale=0.75]  {$X\hat{\beta }$};


\end{tikzpicture}
\end{FigureCenter}


$ X \hat{\beta} $ is the orthogonal projection of $ y $ on $ \operatorname{range}(X) $.

Residual $ e=y-X \hat{\beta} $ is an (observable) random variable.

\subsection{Mean and Covariance of Least Squares Estimate}

\begin{theorem}
    \begin{equation}
\hat{\beta}=X^{\dagger}(X \beta+\epsilon)=\beta+X^{\dagger} \epsilon
\end{equation}
\end{theorem}

\begin{theorem}
    Least squares estimator is unbiased.
    \begin{equation} \mathbf{E} \hat{\beta}=\beta \end{equation}
\end{theorem}

\begin{theorem}
    Covariance matrix of least squares estimate is

\begin{equation}
\begin{aligned}
\mathbf{E}(\hat{\beta}-\beta)(\hat{\beta}-\beta)^{T} &=\mathbf{E}\left(\left(X^{\dagger} \epsilon\right)\left(X^{\dagger} \epsilon\right)^{T}\right) \\
&=\mathbf{E}\left(\left(X^{T} X\right)^{-1} X^{T} \epsilon \epsilon^{T} X\left(X^{T} X\right)^{-1}\right) \\
&=\sigma^{2}\left(X^{T} X\right)^{-1}
\end{aligned}
\end{equation}
\end{theorem}

\begin{theorem}
    covariance of $ \hat{\beta}_{i} $ and $ \hat{\beta}_{j}(i \neq j) $ is
\begin{equation}
\mathbf{E}\left(\left(\hat{\beta}_{i}-\beta_{i}\right)\left(\hat{\beta}_{j}-\beta_{j}\right)\right)=\sigma^{2}\left(\left(X^{T} X\right)^{-1}\right)_{i j}
\end{equation}
\end{theorem}

\begin{corollary}
    For $ i=j $, $
    \begin{aligned}
    \mathbf{E}(\hat{\beta}-\beta)(\hat{\beta}-\beta)^{T} =\sigma^{2}\left(X^{T} X\right)^{-1}
    \end{aligned}
    $ is the variance of $ \hat{\beta}_{i} $.
\end{corollary}



\subsection{Estimate of $ \sigma^{2} $}

% todo (2021-12-17 23:40): Figure

\begin{theorem}
    $ \mathbf{E}\|\epsilon\|^{2}=n \sigma^{2} $
\end{theorem}

\begin{theorem}
    $ \mathbf{E}\|e\|^{2}=(n-p) \sigma^{2} $
\end{theorem}

\begin{theorem}
    $ \mathbf{E}\|X(\hat{\beta}-\beta)\|^{2}=p \sigma^{2} $
\end{theorem}

\begin{definition}[Estimate $ \hat{\sigma} $ of $ \sigma $]
    Define estimate $ \hat{\sigma} $ of $ \sigma $ as
\begin{equation}
\hat{\sigma}=\frac{\|e\|}{\sqrt{n-p}}
\end{equation}
\end{definition}

\begin{theorem}
    $ \hat{\sigma}^{2} $ is an unbiased estimate of $ \sigma^{2} $ :
\begin{equation}
\mathbf{E} \hat{\sigma}^{2}=\frac{1}{n-p} \mathbf{E}\|e\|^{2}=\sigma^{2}
\end{equation}
\end{theorem}


\begin{proof}
    First expression is immediate: $ \mathbf{E}\|\epsilon\|^{2}=\sum_{i=1}^{n} \mathbf{E} \epsilon_{i}^{2}=n \sigma^{2} $

    To show that $ \mathbf{E}\|X(\hat{\beta}-\beta)\|^{2}=p \sigma^{2} $, first note that
\begin{equation}
\begin{aligned}
X(\hat{\beta}-\beta) &=X X^{\dagger} y-X \beta \\
&=X X^{\dagger}(X \beta+\epsilon)-X \beta \\
&=X X^{\dagger} \epsilon \\
&=X\left(X^{T} X\right)^{-1} X^{T} \epsilon
\end{aligned}
\end{equation}

On line 3 we used $ X^{\dagger} X=I $ (however, note that $ X X^{\dagger} \neq I $ if $ X $ is tall).

\begin{theorem}
    squared norm of $ X(\beta-\hat{\beta}) $ is
\begin{equation}
\|X(\hat{\beta}-\beta)\|^{2}=\epsilon^{T}\left(X X^{\dagger}\right)^{2} \epsilon=\epsilon^{T} X X^{\dagger} \epsilon
\end{equation}

(first step uses symmetry of $ X X^{\dagger} $; second step, $ X^{\dagger} X=I $)
\end{theorem}



Expected value of squared norm is
\begin{equation}
\begin{aligned}
\mathbf{E}\|X(\hat{\beta}-\beta)\|^{2}=\mathbf{E}\left(\epsilon^{T} X X^{\dagger} \epsilon\right) &=\sum_{i, j} \mathbf{E}\left(\epsilon_{i} \epsilon_{j}\right)\left(X X^{\dagger}\right)_{i j} \\
&=\sigma^{2} \sum_{i=1}^{n}\left(X X^{\dagger}\right)_{i i} \\
&=\sigma^{2} \sum_{i=1}^{n} \sum_{j=1}^{p} X_{i j}\left(X^{\dagger}\right)_{j i} \\
&=\sigma^{2} \sum_{j=1}^{p}\left(X^{\dagger} X\right)_{j j} \\
&=p \sigma^{2}
\end{aligned}
\end{equation}

expression $ \mathbf{E}\|e\|^{2}=(n-p) \sigma^{2} $ on page $ 9.38 $ now follows from
\begin{equation}
\|\epsilon\|^{2}=\|e+X \hat{\beta}-X \beta\|^{2}=\|e\|^{2}+\|X(\hat{\beta}-\beta)\|^{2}
\end{equation}
\end{proof}


\subsection{Linear Estimator}

\begin{problem}
    linear regression model, with same assumptions as before:
\begin{equation}
y=X \beta+\epsilon
\end{equation}
\end{problem}

\begin{definition}[Linear Estimator]
    A linear estimator of $ \beta $ maps observations $ y $ to the estimate
\begin{equation}
\hat{\beta}=B y
\end{equation}

Estimator is defined by the $ p \times n $ matrix $ B $, least squares estimator is an example with $ B=X^{\dagger} $.
\end{definition}




\subsection{Unbiased Linear Estimator}

\begin{theorem}
    If $ B $ is a left inverse of $ X $, then estimator $ \hat{\beta}=B y $ can be written as:
\begin{equation}
\hat{\beta}=B y=B(X \beta+\epsilon)=\beta+B \epsilon
\end{equation}
\end{theorem}

\begin{corollary}
    This shows that the linear estimator is \term{unbiased} $ (\mathbf{E} \hat{\beta}=\beta) $ if $ B X=I $.
\end{corollary}

\begin{theorem}
    Covariance matrix of unbiased linear estimator is
\begin{equation}
\mathbf{E}\left((\hat{\beta}-\beta)(\hat{\beta}-\beta)^{T}\right)=\mathbf{E}\left(B \epsilon \epsilon^{T} B^{T}\right)=\sigma^{2} B B^{T}
\end{equation}
\end{theorem}

\begin{theorem}
    If $ c $ is a (non-random) $ p $-vector, then estimate $ c^{T} \hat{\beta} $ of $ c^{T} \beta $ has variance
$ \mathbf{E}\left(c^{T} \hat{\beta}-c^{T} \beta\right)^{2}=\sigma^{2} c^{T} B B^{T} c $
\end{theorem}

\begin{corollary}
    Least squares estimator is an example with $ B=X^{\dagger} $ and $ B B^{T}=\left(X^{T} X\right)^{-1} $
\end{corollary}



\section{Best Linear Unbiased Estimator}

\begin{theorem}
    If $ B $ is a left inverse of $ X $ then for all $ p $-vectors $ c $
\begin{equation}
c^{T} B B^{T} c \geq c^{T}\left(X^{T} X\right)^{-1} c
\end{equation}
\end{theorem}


\begin{proof}
    Use $ B X=I $ to write $ B B^{T} $ as
\begin{equation}
\begin{aligned}
B B^{T} &=\left(B-\left(X^{T} X\right)^{-1} X^{T}\right)\left(B^{T}-X\left(X^{T} X\right)^{-1}\right)+\left(X^{T} X\right)^{-1} \\
&=\left(B-X^{\dagger}\right)\left(B-X^{\dagger}\right)^{T}+\left(X^{T} X\right)^{-1}
\end{aligned}
\end{equation}

Hence,
\begin{equation}
\begin{aligned}
c^{T} B B^{T} C &=c^{T}\left(B-X^{\dagger}\right)\left(B-X^{\dagger}\right)^{T} c+c^{T}\left(X^{T} X\right)^{-1} c \\
&=\left\|\left(B-X^{\dagger}\right)^{T} c\right\|^{2}+c^{T}\left(X^{T} X\right)^{-1} c \\
& \geq c^{T}\left(X^{T} X\right)^{-1} c
\end{aligned}
\end{equation}
with equality if $ B=X^{\dagger} $.
\end{proof}

\begin{corollary}
    Left-hand side gives variance of $ c^{T} \hat{\beta} $ for linear unbiased estimator
\begin{equation}
\hat{\beta}=B y
\end{equation}
\end{corollary}

\begin{corollary}
    Right-hand side gives variance of $ c^{T} \hat{\beta}_{\mathrm{ls}} $ for least squares estimator
\begin{equation}
\hat{\beta}_{\mathrm{ls}}=X^{\dagger} y
\end{equation}
\end{corollary}

\begin{theorem}[Gauss-Markov theorem]
    Least squares estimator is the ``best linear unbiased estimator'' (BLUE).
\end{theorem}

This is known as the Gauss-Markov theorem.

