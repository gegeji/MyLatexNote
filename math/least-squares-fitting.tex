\chapter{Least squares data fitting}

\section{Model fitting}


suppose $ x $ and a scalar quantity $ y $ are related as
$$
y \approx f(x)
$$
- $ x $ is the explanatory variable or independent variable
- $ y $ is the outcome, or response variable, or dependent variable
- we don't know $ f $, but have some idea about its general form
Model fitting
- find an approximate model $ \hat{f} $ for $ f $, based on observations
- we use the notation $ \hat{y} $ for the model prediction of the outcome $ y $ :
$$
\hat{y}=\hat{f}(x)
$$


\subsection{Prediction error}

we have data consisting of $ N $ examples (samples, measurements, observations):
$$
x^{(1)}, \ldots, x^{(N)}, \quad y^{(1)}, \ldots, y^{(N)}
$$
- model prediction for example $ i $ is $ \hat{y}^{(i)}=\hat{f}\left(x^{(i)}\right) $
- the prediction error or residual for example $ i $ is
$$
r^{(i)}=y^{(i)}-\hat{y}^{(i)}=y^{(i)}-\hat{f}\left(x^{(i)}\right)
$$
- the model $ \hat{f} $ fits the data well if the $ N $ residuals $ r^{(i)} $ are small
- prediction error can be quantified using the mean square error (MSE)
$$
\frac{1}{N} \sum_{i=1}^{N}\left(r^{(i)}\right)^{2}
$$
the square root of the MSE is the RMS error

\section{Regression}

we first consider the regression model (page 1.30):
$$
\hat{f}(x)=x^{T} \beta+v
$$
- here the independent variable $ x $ is an $ n $-vector
- the elements of $ x $ are the regressors
- the model is parameterized by the weight vector $ \beta $ and the offset (intercept) $ v $
- the prediction error for example $ i $ is
$$
\begin{aligned}
r^{(i)} &=y^{(i)}-\hat{f}\left(x^{(i)}\right) \\
&=y^{(i)}-\left(x^{(i)}\right)^{T} \beta-v
\end{aligned}
$$
- the MSE is
$$
\frac{1}{N} \sum_{i=1}^{N}\left(r^{(i)}\right)^{2}=\frac{1}{N} \sum_{i=1}^{N}\left(y^{(i)}-\left(x^{(i)}\right)^{T} \beta-v\right)^{2}
$$

\section{Least squares regression}

choose the model parameters $ v, \beta $ that minimize the MSE
$$
\frac{1}{N} \sum_{i=1}^{N}\left(v+\left(x^{(i)}\right)^{T} \beta-y^{(i)}\right)^{2}
$$
this is a least squares problem: minimize $ \left\|A \theta-y^{\mathrm{d}}\right\|^{2} $ with
$$
A=\left[\begin{array}{cc}
1 & \left(x^{(1)}\right)^{T} \\
1 & \left(x^{(2)}\right)^{T} \\
\vdots & \vdots \\
1 & \left(x^{(N)}\right)^{T}
\end{array}\right], \quad \theta=\left[\begin{array}{c}
v \\
\beta
\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(N)}
\end{array}\right]
$$
we write the solution as $ \hat{\theta}=(\hat{v}, \hat{\beta}) $

\section{Linear-in-parameters model}

we choose the model $ \hat{f}(x) $ from a family of models
$$
\hat{f}(x)=\theta_{1} f_{1}(x)+\theta_{2} f_{2}(x)+\cdots+\theta_{p} f_{p}(x)
$$
- the functions $ f_{i} $ are scalar valued basis functions (chosen by us)
- the basis functions often include a constant function (typically, $ f_{1}(x)=1 $ )
- the coefficients $ \theta_{1}, \ldots, \theta_{p} $ are the model parameters
- the model $ \hat{f}(x) $ is linear in the parameters $ \theta_{i} $
- if $ f_{1}(x)=1 $, this can be interpreted as a regression model
$$
\hat{y}=\beta^{T} \tilde{x}+v
$$

with parameters $ v=\theta_{1}, \beta=\theta_{2: p} $ and new features $ \tilde{x} $ generated from $ x $ :
$$
\tilde{x}_{1}=f_{2}(x), \quad \ldots, \quad \tilde{x}_{p}=f_{p}(x)
$$

\section{Least squares model fitting}

- fit linear-in-parameters model to data set $ \left(x^{(1)}, y^{(1)}\right), \ldots,\left(x^{(N)}, y^{(N)}\right) $
- residual for data sample $ i $ is
$$
r^{(i)}=y^{(i)}-\hat{f}\left(x^{(i)}\right)=y^{(i)}-\theta_{1} f_{1}\left(x^{(i)}\right)-\cdots-\theta_{p} f_{p}\left(x^{(i)}\right)
$$
- least squares model fitting: choose parameters $ \theta $ by minimizing MSE
$$
\frac{1}{N}\left(\left(r^{(1)}\right)^{2}+\left(r^{(2)}\right)^{2}+\cdots+\left(r^{(N)}\right)^{2}\right)
$$

- this is a least squares problem: minimize $ \left\|A \theta-y^{\mathrm{d}}\right\|^{2} $ with
$$
A=\left[\begin{array}{ccc}
f_{1}\left(x^{(1)}\right) & \cdots & f_{p}\left(x^{(1)}\right) \\
f_{1}\left(x^{(2)}\right) & \cdots & f_{p}\left(x^{(2)}\right) \\
\vdots & & \vdots \\
f_{1}\left(x^{(N)}\right) & \cdots & f_{p}\left(x^{(N)}\right)
\end{array}\right], \quad \theta=\left[\begin{array}{c}
\theta_{1} \\
\theta_{2} \\
\vdots \\
\theta_{p}
\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(N)}
\end{array}\right]
$$

\subsection{Example: polynomial approximation}

$$
\hat{f}(x)=\theta_{1}+\theta_{2} x+\theta_{3} x^{2}+\cdots+\theta_{p} x^{p-1}
$$
- a linear-in-parameters model with basis functions $ 1, x, \ldots, x^{p-1} $
- least squares model fitting: choose parameters $ \theta $ by minimizing MSE
$$
\frac{1}{N}\left(\left(y^{(1)}-\hat{f}\left(x^{(1)}\right)\right)^{2}+\left(y^{(2)}-\hat{f}\left(x^{(2)}\right)\right)^{2}+\cdots+\left(y^{(N)}-\hat{f}\left(x^{(N)}\right)\right)^{2}\right)
$$
- in matrix notation: minimize $ \left\|A \theta-y^{\mathrm{d}}\right\|^{2} $ with
$$
A=\left[\begin{array}{ccccc}
1 & x^{(1)} & \left(x^{(1)}\right)^{2} & \cdots & \left(x^{(1)}\right)^{p-1} \\
1 & x^{(2)} & \left(x^{(2)}\right)^{2} & \cdots & \left(x^{(2)}\right)^{p-1} \\
\vdots & \vdots & \vdots & & \vdots \\
1 & x^{(N)} & \left(x^{(N)}\right)^{2} & \cdots & \left(x^{(N)}\right)^{p-1}
\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(N)}
\end{array}\right]
$$

\section{Piecewise-affine function}

- define knot points $ a_{1}<a_{2}<\cdots<a_{k} $ on the real axis
- piecewise-affine function is continuous, and affine on each interval $ \left[a_{k}, a_{k+1}\right] $
- piecewise-affine function with knot points $ a_{1}, \ldots, a_{k} $ can be written as
$$
\hat{f}(x)=\theta_{1}+\theta_{2} x+\theta_{3}\left(x-a_{1}\right)_{+}+\cdots+\theta_{2+k}\left(x-a_{k}\right)_{+}
$$
where $ u_{+}=\max \{u, 0\} $

\subsection{Piecewise-affine function fitting}

piecewise-affine model is in linear in the parameters $ \theta $, with basis functions
$$
f_{1}(x)=1, \quad f_{2}(x)=x, \quad f_{3}(x)=\left(x-a_{1}\right)_{+}, \quad \ldots, \quad f_{k+2}(x)=\left(x-a_{k}\right)_{+}
$$

\section{Time series trend}

- $ N $ data samples from time series: $ y^{(i)} $ is value at time $ i $, for $ i=1, \ldots, N $
- straight-line fit $ \hat{y}^{(i)}=\theta_{1}+\theta_{2} i $ is the trend line
- $ y^{\mathrm{d}}-\hat{y}^{\mathrm{d}}=\left(y^{(1)}-\hat{y}^{(1)}, \ldots, y^{(N)}-\hat{y}^{(N)}\right) $ is the de-trended time series
- least squares fitting of trend line: minimize $ \left\|A \theta-y^{\mathrm{d}}\right\|^{2} $ with

$$ A=\left[\begin{array}{cc}1 & 1 \\ 1 & 2 \\ 1 & 3 \\ \vdots & \vdots \\ 1 & N\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}y^{(1)} \\ y^{(2)} \\ y^{(3)} \\ \vdots \\ y^{(N)}\end{array}\right] $$

\section{Example: world petroleum consumption}


time series of world petroleum consumption (million barrels/day) versus year

left figure shows data samples and trend line

right figure shows de-trended time series

\section{Trend plus seasonal component}


- model time series as a linear trend plus a periodic component with period $ P $ :
$$
\hat{y}^{\mathrm{d}}=\hat{y}^{\text {lin }}+\hat{y}^{\text {seas }}
$$
with $ \hat{y}^{\operatorname{lin}}=\theta_{1}(1,2, \ldots, N) $ and
$$
\hat{y}^{\text {seas }}=\left(\theta_{2}, \theta_{3}, \ldots, \theta_{P+1}, \theta_{2}, \theta_{3}, \ldots, \theta_{P+1}, \ldots, \theta_{2}, \theta_{3}, \ldots, \theta_{P+1}\right)
$$
- the mean of $ \hat{y}^{\text {seas }} $ serves as a constant offset
- residual $ y^{\mathrm{d}}-\hat{y}^{\mathrm{d}} $ is the de-trended, seasonally adjusted time series
- least squares formulation: minimize $ \left\|A \theta-y^{\mathrm{d}}\right\|^{2} $ with

$$ A_{1: N, 1}=\left[\begin{array}{c}1 \\ 2 \\ \vdots \\ N\end{array}\right], \quad A_{1: N, 2: P+1}=\left[\begin{array}{c}I_{P} \\ I_{P} \\ \vdots \\ I_{P}\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(N)}\end{array}\right] $$

\section{Auto-regressive (AR) time series model}

$$
\hat{z}_{t+1}=\beta_{1} z_{t}+\cdots+\beta_{M} z_{t-M+1}, \quad t=M, M+1, \ldots
$$
- $ z_{1}, z_{2}, \ldots $ is a time series
- $ \hat{z}_{t+1} $ is a prediction of $ z_{t+1} $, made at time $ t $
- prediction $ \hat{z}_{t+1} $ is a linear function of previous $ M $ values $ z_{t}, \ldots, z_{t-M+1} $
- $ M $ is the memory of the model

Least squares fitting of AR model: given oberved data $ z_{1}, \ldots, z_{T} $, minimize
$$
\left(z_{M+1}-\hat{z}_{M+1}\right)^{2}+\left(z_{M+2}-\hat{z}_{M+2}\right)^{2}+\cdots+\left(z_{T}-\hat{z}_{T}\right)^{2}
$$
this is a least squares problem: minimize $ \left\|A \beta-y^{\mathrm{d}}\right\|^{2} $ with
$$
A=\left[\begin{array}{cccc}
z_{M} & z_{M-1} & \cdots & z_{1} \\
z_{M+1} & z_{M} & \cdots & z_{2} \\
\vdots & \vdots & & \vdots \\
z_{T-1} & z_{T-2} & \cdots & z_{T-M}
\end{array}\right], \quad \beta=\left[\begin{array}{c}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{M}
\end{array}\right], \quad y^{\mathrm{d}}=\left[\begin{array}{c}
z_{M+1} \\
z_{M+2} \\
\vdots \\
z_{T}
\end{array}\right]
$$

\section{Generalization and validation}


Generalization ability: ability of model to predict outcomes for new, unseen data
Model validation: to assess generalization ability,
• divide data in two sets: training set and test (or validation) set
• use training set to fit model
• use test set to get an idea of generalization ability
• this is also called out-of-sample validation

Over-fit model

• model with low prediction error on training set, bad generalization ability
• prediction error on training set is much smaller than on test set

\section{Cross-validation}

an extension of out-of-sample validation
- divide data in $ K $ sets (folds); typical values are $ K=5, K=10 $
- for $ i=1 $ to $ K $, fit model $ i $ using fold $ i $ as test set and other data as training set
- compare parameters and train/test RMS errors for the $ K $ models

\section{Boolean (two-way) classification}

- a data fitting problem where the outcome $ y $ can take two values $ +1,-1 $
- values of $ y $ represent two categories (true/false, spam/not spam, ...)
- model $ \hat{y}=\hat{f}(x) $ is called a Boolean classifier
Least squares classifier
- use least squares to fit model $ \tilde{f}(x) $ to training set $ \left(x^{(1)}, y^{(1)}\right), \ldots,\left(x^{(N)}, y^{(N)}\right) $
- $ \tilde{f}(x) $ can be a regression model $ \tilde{f}(x)=x^{T} \beta+v $ or linear in parameters
$$
\tilde{f}(x)=\theta_{1} f_{1}(x)+\cdots+\theta_{p} f_{p}(x)
$$

take sign of $ \tilde{f}(x) $ to get a Boolean classifier
$$
\hat{f}(x)=\operatorname{sign}(\tilde{f}(x))=\left\{\begin{array}{ll}
+1 & \text { if } \tilde{f}(x) \geq 0 \\
-1 & \text { if } \tilde{f}(x)<0
\end{array}\right.
$$

\subsection{Example: handwritten digit classification}

- MNIST data set used in homework
- $ 28 \times 28 $ images of handwritten digits $ \left(n=28^{2}=784\right. $ pixels)
- data set contains 60000 training examples; 10000 test examples
- we only use the 493 pixels that are nonzero in at least 600 training examples
- Boolean classifier distinguishes digit zero $ (y=1) $ from other digits $ (y=-1) $

\subsubsection{Classifier with basic regression model}

$$
\hat{f}(x)=\operatorname{sign}(\tilde{f}(x))=\operatorname{sign}\left(x^{T} \beta+v\right)
$$
- $ x $ is vector of 493 pixel intensities
- figure shows distribution of $ \tilde{f}\left(x^{(i)}\right)=\left(x^{(i)}\right)^{T} \hat{\beta}+\hat{v} $ on training set

blue bars to the left of dashed line are false negatives (misclassified digits zero)

• red bars to the right of dashed line are false positives (misclassified non-zeros)

\subsubsection{Classifier with additional nonlinear features}

$$
\hat{f}(x)=\operatorname{sign}(\tilde{f}(x))=\operatorname{sign}\left(\sum_{i=1}^{p} \theta_{i} f_{i}(x)\right)
$$
- basis functions include constant, 493 elements of $ x $, plus 5000 functions
$$
f_{i}(x)=\max \left\{0, r_{i}^{T} x+s_{i}\right\} \quad \text { with randomly generated } r_{i}, s_{i}
$$
- figure shows distribution of $ \tilde{f}\left(x^{(i)}\right) $ on training set

\section{Multi-class classification}

- a data fitting problem where the outcome $ y $ can takes values $ 1, \ldots, K $
- values of $ y $ represent $ K $ labels or categories
- multi-class classifier $ \hat{y}=\hat{f}(x) $ maps $ x $ to an element of $ \{1,2, \ldots, K\} $
Least squares multi-class classifier
- for $ k=1, \ldots, K $, compute Boolean classifier to distinguish class $ k $ from not $ k $
$$
\hat{f}_{k}(x)=\operatorname{sign}\left(\tilde{f}_{k}(x)\right)
$$
- define multi-class classifier as
$$
\hat{f}(x)=\underset{k=1, \ldots, K}{\operatorname{argmax}} \tilde{f}_{k}(x)
$$

\section{statistics interpretation}

$$
y=X \beta+\epsilon
$$
- $ \beta $ is (non-random) $ p $-vector of unknown parameters
- $ X $ is $ n \times p $ (data matrix or design matrix, i.e., result of experiment design)
- if there is an offset $ v $, we include it in $ \beta $ and add a column of ones in $ X $
- $ \epsilon $ is a random $ n $-vector (random error or disturbance)
- $ y $ is an observable random $ n $-vector
- this notation differs from previous sections but is common in statistics
- we discuss methods for estimating parameters $ \beta $ from observations of $ y $

\subsection{Assumptions}

- $ X $ is tall $ (n>p) $ with linearly independent columns
- random disturbances $ \epsilon_{i} $ have zero mean
$$
\mathbf{E} \epsilon_{i}=0 \quad \text { for } i=1, \ldots, n
$$
- random disturbances have equal variances $ \sigma^{2} $
$$
\mathbf{E} \epsilon_{i}^{2}=\sigma^{2} \quad \text { for } i=1, \ldots, n
$$
- random disturbances are uncorrelated (have zero covariances)
$$
\mathbf{E}\left(\epsilon_{i} \epsilon_{j}\right)=0 \quad \text { for } i, j=1, \ldots, n \text { and } i \neq j
$$
last three assumptions can be combined using matrix and vector notation:
$$
\mathbf{E} \epsilon=0, \quad \mathbf{E} \epsilon \epsilon^{T}=\sigma^{2} I
$$

\subsection{Least squares estimator}

least squares estimate $ \hat{\beta} $ of parameters $ \beta $, given the observations $ y $, is
$$
\hat{\beta}=X^{\dagger} y=\left(X^{T} X\right)^{-1} X^{T} y
$$

% todo (2021-12-17 23:39): figure

- $ X \hat{\beta} $ is the orthogonal projection of $ y $ on $ \operatorname{range}(X) $
- residual $ e=y-X \hat{\beta} $ is an (observable) random variable

\subsection{Mean and covariance of least squares estimate}

$$
\hat{\beta}=X^{\dagger}(X \beta+\epsilon)=\beta+X^{\dagger} \epsilon
$$
- least squares estimator is unbiased: $ \mathbf{E} \hat{\beta}=\beta $
- covariance matrix of least squares estimate is

$$
\begin{aligned}
\mathbf{E}(\hat{\beta}-\beta)(\hat{\beta}-\beta)^{T} &=\mathbf{E}\left(\left(X^{\dagger} \epsilon\right)\left(X^{\dagger} \epsilon\right)^{T}\right) \\
&=\mathbf{E}\left(\left(X^{T} X\right)^{-1} X^{T} \epsilon \epsilon^{T} X\left(X^{T} X\right)^{-1}\right) \\
&=\sigma^{2}\left(X^{T} X\right)^{-1}
\end{aligned}
$$
- covariance of $ \hat{\beta}_{i} $ and $ \hat{\beta}_{j}(i \neq j) $ is
$$
\mathbf{E}\left(\left(\hat{\beta}_{i}-\beta_{i}\right)\left(\hat{\beta}_{j}-\beta_{j}\right)\right)=\sigma^{2}\left(\left(X^{T} X\right)^{-1}\right)_{i j}
$$
for $ i=j $, this is the variance of $ \hat{\beta}_{i} $

\subsection{Estimate of $ \sigma^{2} $}

% todo (2021-12-17 23:40): Figure

$ \mathbf{E}\|\epsilon\|^{2}=n \sigma^{2} $
$ \mathbf{E}\|e\|^{2}=(n-p) \sigma^{2} $
$ \mathbf{E}\|X(\hat{\beta}-\beta)\|^{2}=p \sigma^{2} $

- define estimate $ \hat{\sigma} $ of $ \sigma $ as
$$
\hat{\sigma}=\frac{\|e\|}{\sqrt{n-p}}
$$
- $ \hat{\sigma}^{2} $ is an unbiased estimate of $ \sigma^{2} $ :
$$
\mathbf{E} \hat{\sigma}^{2}=\frac{1}{n-p} \mathbf{E}\|e\|^{2}=\sigma^{2}
$$

\begin{proof}
    first expression is immediate: $ \mathbf{E}\|\epsilon\|^{2}=\sum_{i=1}^{n} \mathbf{E} \epsilon_{i}^{2}=n \sigma^{2} $
- to show that $ \mathbf{E}\|X(\hat{\beta}-\beta)\|^{2}=p \sigma^{2} $, first note that
$$
\begin{aligned}
X(\hat{\beta}-\beta) &=X X^{\dagger} y-X \beta \\
&=X X^{\dagger}(X \beta+\epsilon)-X \beta \\
&=X X^{\dagger} \epsilon \\
&=X\left(X^{T} X\right)^{-1} X^{T} \epsilon
\end{aligned}
$$
on line 3 we used $ X^{\dagger} X=I $ (however, note that $ X X^{\dagger} \neq I $ if $ X $ is tall)
- squared norm of $ X(\beta-\hat{\beta}) $ is
$$
\|X(\hat{\beta}-\beta)\|^{2}=\epsilon^{T}\left(X X^{\dagger}\right)^{2} \epsilon=\epsilon^{T} X X^{\dagger} \epsilon
$$
first step uses symmetry of $ X X^{\dagger} $; second step, $ X^{\dagger} X=I $

- expected value of squared norm is
$$
\begin{aligned}
\mathbf{E}\|X(\hat{\beta}-\beta)\|^{2}=\mathbf{E}\left(\epsilon^{T} X X^{\dagger} \epsilon\right) &=\sum_{i, j} \mathbf{E}\left(\epsilon_{i} \epsilon_{j}\right)\left(X X^{\dagger}\right)_{i j} \\
&=\sigma^{2} \sum_{i=1}^{n}\left(X X^{\dagger}\right)_{i i} \\
&=\sigma^{2} \sum_{i=1}^{n} \sum_{j=1}^{p} X_{i j}\left(X^{\dagger}\right)_{j i} \\
&=\sigma^{2} \sum_{j=1}^{p}\left(X^{\dagger} X\right)_{j j} \\
&=p \sigma^{2}
\end{aligned}
$$

- expression $ \mathbf{E}\|e\|^{2}=(n-p) \sigma^{2} $ on page $ 9.38 $ now follows from
$$
\|\epsilon\|^{2}=\|e+X \hat{\beta}-X \beta\|^{2}=\|e\|^{2}+\|X(\hat{\beta}-\beta)\|^{2}
$$
\end{proof}


\subsection{Linear estimator}

linear regression model (page 9.34), with same assumptions as before (p. 9.35):
$$
y=X \beta+\epsilon
$$
a linear estimator of $ \beta $ maps observations $ y $ to the estimate
$$
\hat{\beta}=B y
$$
- estimator is defined by the $ p \times n $ matrix $ B $
- least squares estimator is an example with $ B=X^{\dagger} $

\subsection{Unbiased linear estimator}

if $ B $ is a left inverse of $ X $, then estimator $ \hat{\beta}=B y $ can be written as:
$$
\hat{\beta}=B y=B(X \beta+\epsilon)=\beta+B \epsilon
$$
- this shows that the linear estimator is unbiased $ (\mathbf{E} \hat{\beta}=\beta) $ if $ B X=I $
- covariance matrix of unbiased linear estimator is
$$
\mathbf{E}\left((\hat{\beta}-\beta)(\hat{\beta}-\beta)^{T}\right)=\mathbf{E}\left(B \epsilon \epsilon^{T} B^{T}\right)=\sigma^{2} B B^{T}
$$

- if $ c $ is a (non-random) $ p $-vector, then estimate $ c^{T} \hat{\beta} $ of $ c^{T} \beta $ has variance
$ \mathbf{E}\left(c^{T} \hat{\beta}-c^{T} \beta\right)^{2}=\sigma^{2} c^{T} B B^{T} c $
least squares estimator is an example with $ B=X^{\dagger} $ and $ B B^{T}=\left(X^{T} X\right)^{-1} $

\section{Best linear unbiased estimator}

if $ B $ is a left inverse of $ X $ then for all $ p $-vectors $ c $
$$
c^{T} B B^{T} c \geq c^{T}\left(X^{T} X\right)^{-1} c
$$
(proof on next page)
- left-hand side gives variance of $ c^{T} \hat{\beta} $ for linear unbiased estimator
$$
\hat{\beta}=B y
$$
- right-hand side gives variance of $ c^{T} \hat{\beta}_{\mathrm{ls}} $ for least squares estimator
$$
\hat{\beta}_{\mathrm{ls}}=X^{\dagger} y
$$
- least squares estimator is the "best linear unbiased estimator" (BLUE)
this is known as the Gauss-Markov theorem

\begin{proof}
    - use $ B X=I $ to write $ B B^{T} $ as
$$
\begin{aligned}
B B^{T} &=\left(B-\left(X^{T} X\right)^{-1} X^{T}\right)\left(B^{T}-X\left(X^{T} X\right)^{-1}\right)+\left(X^{T} X\right)^{-1} \\
&=\left(B-X^{\dagger}\right)\left(B-X^{\dagger}\right)^{T}+\left(X^{T} X\right)^{-1}
\end{aligned}
$$
- hence,
$$
\begin{aligned}
c^{T} B B^{T} C &=c^{T}\left(B-X^{\dagger}\right)\left(B-X^{\dagger}\right)^{T} c+c^{T}\left(X^{T} X\right)^{-1} c \\
&=\left\|\left(B-X^{\dagger}\right)^{T} c\right\|^{2}+c^{T}\left(X^{T} X\right)^{-1} c \\
& \geq c^{T}\left(X^{T} X\right)^{-1} c
\end{aligned}
$$
with equality if $ B=X^{\dagger} $
\end{proof}