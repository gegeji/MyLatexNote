\chapter{Cholesky Factorization}

\section{Positive Definite Matrices}

\begin{definition}[Positive semidefinite]
    a symmetric matrix $ A \in \mathfrak{R}^{n \times n} $ is positive semidefinite if $ x^{T} A x \geq 0 \quad $ for all $ x $

\end{definition}

\begin{definition}[positive definite]
    a symmetric matrix $ A \in \mathfrak{R}^{n \times n} $ is positive definite if $ x^{T} A x>0 \quad $ for all $ x \neq 0 $
\end{definition}

\section{Schur complement}

\begin{definition}[Schur complement]
    partition $ n \times n $ symmetric matrix $ A $ as
\begin{equation}
A=\left[\begin{array}{cc}
A_{11} & A_{2: n, 1}^{T} \\
A_{2: n, 1} & A_{2: n, 2: n}
\end{array}\right]
\end{equation}

the Schur complement of $ A_{11} $ is defined as the $ (n-1) \times(n-1) $ matrix
\begin{equation}
S=A_{2: n, 2: n}-\frac{1}{A_{11}} A_{2: n, 1} A_{2: n, 1}^{T}
\end{equation}
\end{definition}

\begin{theorem}
    if $ A $ is positive definite, then $ S $ is positive definite
\end{theorem}

\begin{proof}
    Take any $ x \neq 0 $ and define $ y=-\left(A_{2: n, 1}^{T} x\right) / A_{11} $; then
\begin{equation}
x^{T} S x=\left[\begin{array}{l}
y \\
x
\end{array}\right]^{T}\left[\begin{array}{cc}
A_{11} & A_{2: n, 1}^{T} \\
A_{2: n, 1} & A_{2: n, 2: n}
\end{array}\right]\left[\begin{array}{l}
y \\
x
\end{array}\right]>0
\end{equation}
because $ A $ is positive definite
\end{proof}

\begin{theorem}
    Positive definite matrices are nonsingular.
\end{theorem}

\begin{theorem}
    If $A$ is positive semidefinite, but not positive definite, then it is singular.
\end{theorem}

\begin{proof}
    to see this, suppose $ A $ is positive semidefinite but not positive definite

    there exists a nonzero $ x $ with $ x^{T} A x=0 $


since $ A $ is positive semidefinite the following function is nonnegative:
\begin{equation}
\begin{aligned}
f(t) &=(x-t A x)^{T} A(x-t A x) \\
&=x^{T} A x-2 t x^{T} A^{2} x+t^{2} x^{T} A^{3} x \\
&=-2 t\|A x\|^{2}+t^{2} x^{T} A^{3} x
\end{aligned}
\end{equation}

$ f(t) \geq 0 $ for all $ t $ is only possible if $ \|A x\|=0 $, i.e., $ A x=0 $

hence there exists a nonzero $ x $ with $ A x=0 $, so $ A $ is singular
\end{proof}

\begin{corollary}
    if $ A \in \mathfrak{R}^{n \times n} $ is positive semidefinite, then
\begin{equation}
B^{T} A B
\end{equation}
is positive semidefinite for any $ B \in \mathfrak{R}^{n \times m} $
\end{corollary}

\begin{corollary}
    if $ A \in \mathfrak{R}^{n \times n} $ is positive definite, then
\begin{equation}
B^{T} A B
\end{equation}
is positive definite for any $ B \in \mathfrak{R}^{n \times m} $ with linearly independent columns
\end{corollary}

\section{Examples}

\subsection{Graph Laplacian}

\begin{definition}[Laplacian of the Graph]
    the positive semidefinite matrix $ A=B B^{T} $ is called the Laplacian of the graph
\begin{equation}
A_{i j}=\left\{\begin{array}{ll}
\text { degree of node } i & \text { if } i=j \\
-1 & \text { if } i \neq j \text { and there is an } \operatorname{arc} i \rightarrow j \text { or } j \rightarrow i \\
0 & \text { otherwise }
\end{array}\right.
\end{equation}
the degree of a node is the number of arcs incident to it
\end{definition}

\begin{theorem}
    if $ y $ is vector of node potentials, then $ B^{T} y $ contains potential differences:
$ \left(B^{T} y\right)_{j}=y_{k}-y_{l} \quad $ if arc $ j $ goes from node $ l $ to $ k $
\end{theorem}

\begin{theorem}[Dirichlet energy function]
    Define $ y^{T} A y=y^{T} B B^{T} y $ is the sum of squared potential differences
\begin{equation}
y^{T} A y=\left\|B^{T} y\right\|^{2}=\sum_{\operatorname{arcs} i \rightarrow j}\left(y_{j}-y_{i}\right)^{2}
\end{equation}

This is also known as the Dirichlet energy function.
\end{theorem}

\section{Cholesky Factorization}

\begin{theorem}
    every positive definite matrix $ A \in \mathfrak{R}^{n \times n} $ can be factored as
\begin{equation}
A=R^{T} R
\end{equation}
where $ R $ is upper triangular with positive diagonal elements, $ R $ is called the Cholesky factor of $ A $.
\end{theorem}

complexity of computing $ R $ is $ (1 / 3) n^{3} $ flops.

It can be interpreted as "square root" of a positive definite matrix and gives a practical method for testing positive definiteness.

\begin{algorithm}
    \caption{Cholesky factorization of order $ n-1 $}
    \KwIn{\begin{equation} \begin{aligned}\left[\begin{array}{cc}A_{11} & A_{1,2: n} \\ A_{2: n, 1} & A_{2: n, 2: n}\end{array}\right] &=\left[\begin{array}{cc}R_{11} & 0 \\ R_{1,2: n}^{T} & R_{2: n, 2: n}^{T}\end{array}\right]\left[\begin{array}{cc}R_{11} & R_{1,2: n} \\ 0 & R_{2: n, 2: n}\end{array}\right] \\ &=\left[\begin{array}{cc}R_{11}^{2} & R_{11} R_{1,2: n} \\ R_{11} R_{1,2: n}^{T} & R_{1,2: n}^{T} R_{1,2: n}+R_{2: n, 2: n}^{T} R_{2: n, 2: n}\end{array}\right] \end{aligned} \end{equation}}
    
compute first row of $ R $ :
\begin{equation}
R_{11}=\sqrt{A_{11}}, \quad R_{1,2: n}=\frac{1}{R_{11}} A_{1,2: n}
\end{equation}\;
compute 2,2 block $ R_{2: n, 2: n} $ from
\begin{equation}
A_{2: n, 2: n}-R_{1,2: n}^{T} R_{1,2: n}=R_{2: n, 2: n}^{T} R_{2: n, 2: n}
\end{equation}

\end{algorithm}

\begin{proposition}
    the algorithm works for positive definite $ A $ of size $ n \times n $
\end{proposition}

\begin{proof}
    if $ A $ is positive definite then $ A_{11}>0 $.

    if $ A $ is positive definite, then
\begin{equation}
A_{2: n, 2: n}-R_{1,2: n}^{T} R_{1,2: n}=A_{2: n, 2: n}-\frac{1}{A_{11}} A_{2: n, 1} A_{2: n, 1}^{T}
\end{equation}
is positive definite.

Hence the algorithm works for $ n=m $ if it works for $ n=m-1 $.

It obviously works for $ n=1 $; therefore it works for all $ n $.
\end{proof}


\section{Examples for Cholesky Factorization}

\begin{example}
    \begin{equation} \begin{aligned}\left[\begin{array}{rrr}25 & 15 & -5 \\ 15 & 18 & 0 \\ -5 & 0 & 11\end{array}\right]=&\left[\begin{array}{ccc}R_{11} & 0 & 0 \\ R_{12} & R_{22} & 0 \\ R_{13} & R_{23} & R_{33}\end{array}\right]\left[\begin{array}{ccc}R_{11} & R_{12} & R_{13} \\ 0 & R_{22} & R_{23} \\ 0 & 0 & R_{33}\end{array}\right] \\=&\left[\begin{array}{rrr}5 & 0 & 0 \\ 3 & 3 & 0 \\ -1 & 1 & 3\end{array}\right]\left[\begin{array}{rrr}5 & 3 & -1 \\ 0 & 3 & 1 \\ 0 & 0 & 3\end{array}\right] \end{aligned} \end{equation}

    \begin{equation}
\left[\begin{array}{rrr}
25 & 15 & -5 \\
15 & 18 & 0 \\
-5 & 0 & 11
\end{array}\right]=\left[\begin{array}{ccc}
R_{11} & 0 & 0 \\
R_{12} & R_{22} & 0 \\
R_{13} & R_{23} & R_{33}
\end{array}\right]\left[\begin{array}{ccc}
R_{11} & R_{12} & R_{13} \\
0 & R_{22} & R_{23} \\
0 & 0 & R_{33}
\end{array}\right]
\end{equation}
first row of $ R $
\begin{equation}
\left[\begin{array}{rrr}
25 & 15 & -5 \\
15 & 18 & 0 \\
-5 & 0 & 11
\end{array}\right]=\left[\begin{array}{rcc}
5 & 0 & 0 \\
3 & R_{22} & 0 \\
-1 & R_{23} & R_{33}
\end{array}\right]\left[\begin{array}{ccc}
5 & 3 & -1 \\
0 & R_{22} & R_{23} \\
0 & 0 & R_{33}
\end{array}\right]
\end{equation}

second row of $ R $
\begin{equation}\displaystyle \begin{aligned}
    \left[\begin{array}{ r r }
    18 & 0\\
    0 & 11
    \end{array}\right] -\left[\begin{array}{ r }
    3\\
    -1
    \end{array}\right]\left[\begin{array}{ l l }
    3 & -1
    \end{array}\right] & =\left[\begin{array}{ c c }
    R_{22} & 0\\
    R_{23} & R_{33}
    \end{array}\right]\left[\begin{array}{ c c }
    R_{22} & R_{23}\\
    0 & R_{33}
    \end{array}\right]\\
    \left[\begin{array}{ r r }
    9 & 3\\
    3 & 10
    \end{array}\right] & =\left[\begin{array}{ c c }
    3 & 0\\
    1 & R_{33}
    \end{array}\right]\left[\begin{array}{ c c }
    3 & 1\\
    0 & R_{33}
    \end{array}\right]
    \end{aligned}\end{equation}

third column of $ R: 10-1=R_{33}^{2}, i . e ., R_{33}=3 $
\end{example}

\section{Solving equations with positive definite $A$}

\begin{problem}
    solve $ A x=b $ with $ A $ a positive definite $ n \times n $ matrix
\end{problem}

\begin{algorithm}[htbp]
    \caption{Solving equations with positive definite $A$}
    factor $ A $ as $ A=R^{T} R $\;
    solve $ R^{T} R x=b $\;
    solve $ R^{T} y=b $ by forward substitution\;
    solve $ R x=y $ by back substitution\;
\end{algorithm}


Complexity: $ (1 / 3) n^{3}+2 n^{2} \approx(1 / 3) n^{3} $ flops

\begin{itemize}
    \item factorization: $ (1 / 3) n^{3} $
    \item forward and backward substitution: $ 2 n^{2} $
\end{itemize}


\section{Cholesky Factorization of Gram Matrix}
Suppose $ B $ is an $ m \times n $ matrix with linearly independent colum, and the Gram matrix $ A=B^{T} B $ is positive definite.

There two methods for computing the Cholesky factor of $ A $, given $ B $

\begin{algorithm}
    \caption{computing the Cholesky factor of $ A $}
    \KwOut{matrix $ R $ (the Cholesky factor of $ A $)}
    compute $ A=B^{T} B $\;
    compute Cholesky factorization of $ A $
    \begin{equation}
    A=R^{T} R
    \end{equation}\;
    
\end{algorithm}

\begin{algorithm}
    \caption{computing the Cholesky factor of $ A $}
    \KwOut{matrix $ R $ (the Cholesky factor of $ A $)}

compute QR factorization $ B=Q R $; since
    \begin{equation}
    A=B^{T} B=R^{T} Q^{T} Q R=R^{T} R
    \end{equation}\;
\end{algorithm}

\begin{example}
    \begin{equation}
B=\left[\begin{array}{rr}
3 & -6 \\
4 & -8 \\
0 & 1
\end{array}\right], \quad A=B^{T} B=\left[\begin{array}{rr}
25 & -50 \\
-50 & 101
\end{array}\right]
\end{equation}
1. Cholesky factorization:
\begin{equation}
A=\left[\begin{array}{rr}
5 & 0 \\
-10 & 1
\end{array}\right]\left[\begin{array}{rr}
5 & -10 \\
0 & 1
\end{array}\right]
\end{equation}
2. QR factorization
\begin{equation}
B=\left[\begin{array}{rr}
3 & -6 \\
4 & -8 \\
0 & 1
\end{array}\right]=\left[\begin{array}{rr}
3 / 5 & 0 \\
4 / 5 & 0 \\
0 & 1
\end{array}\right]\left[\begin{array}{rr}
5 & -10 \\
0 & 1
\end{array}\right]
\end{equation}
\end{example}

\subsection{Comparison of the two methods}

Numerical stability: QR factorization method is more stable

\begin{itemize}
    \item QR method computes $ R $ without "squaring" $ B $ (i.e., forming $ B^{T} B $ )
    \item this is important when the columns of $ B $ are "almost" linearly dependent
\end{itemize}

Complexity:
\begin{itemize}
    \item method 1: cost of symmetric product $ B^{T} B $ plus Cholesky factorization
\begin{equation}
m n^{2}+(1 / 3) n^{3} \text { flops }
\end{equation}
    \item method 2: $ 2 m n^{2} $ flops for QR factorization
    \item method 1 is faster but only by a factor of at most two (if $ m \gg n $ )
\end{itemize}


\section{Sparse positive definite matrices}

Cholesky factorization of dense matrices:
\begin{itemize}
    \item $ (1 / 3) n^{3} $ flops
    \item on a standard computer: a few seconds or less, for $ n $ up to several 1000
\end{itemize}

Cholesky factorization of sparse matrices:
\begin{itemize}
    \item if $ A $ is very sparse, $ R $ is often (but not always) sparse
    \item if $ R $ is sparse, the cost of the factorization is much less than (1/3) $ n^{3} $
    \item exact cost depends on $ n $, number of nonzero elements, sparsity pattern
    \item very large sets of equations can be solved by exploiting sparsity
\end{itemize}

\section{Sparse Cholesky factorization}

\begin{theorem}
    if $ A $ is sparse and positive definite, it is usually factored as
\begin{equation}
A=P R^{T} R P^{T}
\end{equation}

$ P $ a permutation matrix; $ R $ upper triangular with positive diagonal elements
\end{theorem}

Interpretation: we permute the rows and columns of $ A $ and factor
\begin{equation}
P^{T} A P=R^{T} R
\end{equation}

\begin{itemize}
    \item choice of permutation greatly affects the sparsity $ R $
    \item there exist several heuristic methods for choosing a good permutation
\end{itemize}


\section{Solving sparse positive definite equations}

solve $ A x=b $ with $ A $ a sparse positive definite matrix

\begin{algorithm}
    \caption{Solving sparse positive definite equations}
    compute sparse Cholesky factorization $ A=P R^{T} R P^{T} $\;
    permute right-hand side: $ c:=P^{T} b $\;
    solve $ R^{T} y=c $ by forward substitution\;
    solve $ R z=y $ by back substitution\;
    permute solution: $ x:=P z $\;
\end{algorithm}

\section{Quadratic form}

\begin{theorem}
    suppose $A$ is $n \times n$ and Hermitian $\left(A_{i j}=\bar{A}_{j i}\right)$
\begin{equation}
\begin{aligned}
x^{H} A x &=\sum_{i=1}^{n} \sum_{j=1}^{n} A_{i j} \bar{x}_{i} x_{j} \\
&=\sum_{i=1}^{n} A_{i i}\left|x_{i}\right|^{2}+\sum_{i>j}\left(A_{i j} \bar{x}_{i} x_{j}+\bar{A}_{i j} x_{i} \bar{x}_{j}\right) \\
&=\sum_{i=1}^{n} A_{i i}\left|x_{i}\right|^{2}+2 \operatorname{Re} \sum_{i>j} A_{i j} \bar{x}_{i} x_{j}
\end{aligned}
\end{equation}
\end{theorem}

\begin{remark}
    note that $x^{H} A x$ is real for all $x \in \mathbf{C}^{n}$
\end{remark}




\section{Complex positive definite matrices}

\begin{definition}
    a Hermitian $n \times n$ matrix $A$ is positive semidefinite if
\begin{equation}
x^{H} A x \geq 0 \quad \text { for all } x \in \mathbf{C}^{n}
\end{equation}
\end{definition}

\begin{definition} Hermitian $n \times n$ matrix $A$ is positive definite if
\begin{equation}
x^{H} A x>0 \text { for all nonzero } x \in \mathbf{C}^{n}
\end{equation}
\end{definition}

\begin{theorem}[Cholesky factorization of complex matrices]
    every positive definite matrix $A \in \mathbf{C}^{n \times n}$ can be factored as
\begin{equation}
A=R^{H} R
\end{equation}
where $R$ is upper triangular with positive real diagonal elements
\end{theorem}


\section{Regularized least squares model fitting}

We revisit the data fitting problem with linear-in-parameters model

\begin{problem}[Data fitting problem (Regularized least squares model fitting)]
    
\begin{equation}\text{ minimize } \sum_{k=1}^{N}\left(\theta^{T} F\left(x^{(k)}\right)-y^{(k)}\right)^{2}+\lambda \sum_{j=1}^{p} \theta_{j}^{2}\end{equation}

 where  \begin{equation}
    \begin{aligned}
    \hat{f}(x) &=\theta_{1} f_{1}(x)+\theta_{2} f_{2}(x)+\cdots+\theta_{p} f_{p}(x) \\
    &=\theta^{T} F(x)
    \end{aligned}
    \end{equation}

    $F(x)=\left(f_{1}(x), \ldots, f_{p}(x)\right)$ is a $p$-vector of basis functions, $f_{1}(x), \ldots, f_{p}(x)$, $\left(x^{(1)}, y^{(1)}\right)$, $\ldots$,$\left(x^{(N)}, y^{(N)}\right)$ are $N$ examples.
\end{problem}

To simplify notation, we add regularization for all coefficients $\theta_{1}, \ldots, \theta_{p}$

Next discussion can be modified to handle $f_{1}(x)=1$, regularization $\sum_{j=2}^{p} \theta_{j}^{2}$.

\begin{problem}
    \begin{equation}\text{minimize} \quad\|A \theta-b\|^{2}+\lambda\|\theta\|^{2}\end{equation}
\end{problem}

A has size $N \times p$ (number of examples $\times$ number of basis functions)

\begin{equation}
A=\left[\begin{array}{c}
F\left(x^{(1)}\right)^{T} \\
F\left(x^{(2)}\right)^{T} \\
\vdots \\
F\left(x^{(N)}\right)^{T}
\end{array}\right]=\left[\begin{array}{cccc}
f_{1}\left(x^{(1)}\right) & f_{2}\left(x^{(1)}\right) & \cdots & f_{p}\left(x^{(1)}\right) \\
f_{1}\left(x^{(2)}\right) & f_{2}\left(x^{(2)}\right) & \cdots & f_{p}\left(x^{(2)}\right) \\
\vdots & \vdots & & \vdots \\
f_{1}\left(x^{(N)}\right) & f_{2}\left(x^{(N)}\right) & \cdots & f_{p}\left(x^{(N)}\right)
\end{array}\right]
\end{equation}
,$b$ is the $N$-vector $b=\left(y^{(1)}, \ldots, y^{(N)}\right)$

We discuss methods for problems with $N \ll p$ ( $A$ is very wide).

\begin{theorem}
    The equivalent "stacked" least squares problem has size $(p+N) \times p$
\end{theorem}

\begin{remark}
    QR factorization method may be too expensive when $N \ll p$.
\end{remark}

\begin{theorem}
    from the normal equations:
\begin{equation}
\hat{\theta}=\left(A^{T} A+\lambda I\right)^{-1} A^{T} b=A^{T}\left(A A^{T}+\lambda I\right)^{-1} b
\end{equation}

second expression follows from the "push-through" identity
\begin{equation}
\left(A^{T} A+\lambda I\right)^{-1} A^{T}=A^{T}\left(A A^{T}+\lambda I\right)^{-1}
\end{equation}
\end{theorem}

\begin{proof}
    this is easily proved, by writing it as $A^{T}\left(A A^{T}+\lambda I\right)=\left(A^{T} A+\lambda I\right) A^{T}$.

    from the second expression for $\hat{\theta}$ and the definition of $A$,

    \begin{equation}
    \hat{f}(x)=\hat{\theta}^{T} F(x)=w^{T} A F(x)=\sum_{i=1}^{N} w_{i} F\left(x^{(i)}\right)^{T} F(x)
    \end{equation}

    where $w=\left(A A^{T}+\lambda I\right)^{-1} b$.
\end{proof}


\begin{algorithm}
    \caption{Regularized least squares model fitting}
    1. compute the $N \times N$ matrix $Q=A A^{T}$, which has elements
\begin{equation}
Q_{i j}=F\left(x^{(i)}\right)^{T} F\left(x^{(j)}\right), \quad i, j=1, \ldots, N
\end{equation}
2. use a Cholesky factorization to solve the equation
\begin{equation}
(Q+\lambda I) w=b
\end{equation}
\end{algorithm}

\begin{remark}
    $\hat{\theta}=A^{T} w$ is not needed; $w$ is sufficient to evaluate the function $\hat{f}(x)$ :
\begin{equation}
\hat{f}(x)=\sum_{i=1}^{N} w_{i} F\left(x^{(i)}\right)^{T} F(x)
\end{equation}
\end{remark}

complexity: (1/3) $N^{3}$ flops for factorization plus cost of computing $Q$.

\subsection{Example: multivariate polynomials}

\begin{definition}
    $\hat{f}(x)$ is a polynomial of degree $d$ (or less) in $n$ variables $x=\left(x_{1}, \ldots, x_{n}\right)$

    $\hat{f}(x)$ is a linear combination of all possible monomials
    \begin{equation}
    x_{1}^{k_{1}} x_{2}^{k_{2}} \cdots x_{n}^{k_{n}}
    \end{equation}
    where $k_{1}, \ldots, k_{n}$ are nonnegative integers with $k_{1}+k_{2}+\cdots+k_{n} \leq d$
\end{definition}

\begin{theorem}
    number of different monomials is
\begin{equation}
\left(\begin{array}{c}
n+d \\
n
\end{array}\right)=\frac{(n+d) !}{n ! d !}
\end{equation}
\end{theorem}

\begin{example}
    for $n=2, d=3$ there are ten monomials

\begin{equation}1,  x_{1},  x_{2},  x_{1}^{2},  x_{1} x_{2},  x_{2}^{2},  x_{1}^{3},  x_{1}^{2} x_{2},  x_{1} x_{2}^{2},  x_{2}^{3}\end{equation}
\end{example}


\section{Multinomial formula}

\begin{theorem}
    \begin{equation}
\left(x_{0}+x_{1}+\cdots+x_{n}\right)^{d}=\sum_{k_{0}+\cdots+k_{n}=d} \frac{(d+1) !}{k_{0} ! k_{1} ! \cdots k_{n} !} x_{0}^{k_{0}} x_{1}^{k_{1}} \cdots x_{n}^{k_{n}}
\end{equation}

sum is over all nonnegative integers $k_{0}, k_{1}, \ldots, k_{n}$ with sum $d$
\end{theorem}

\begin{corollary}
    setting $x_{0}=1$ gives
\begin{equation}
\left(1+x_{1}+x_{2}+\cdots+x_{n}\right)^{d}=\sum_{k_{1}+\cdots+k_{n} \leq d} c_{k_{1} k_{2} \cdots k_{n}} x_{1}^{k_{1}} x_{2}^{k_{2}} \cdots x_{n}^{k_{n}}
\end{equation}
\end{corollary}

the sum includes all monomials of degree $d$ or less with variables $x_{1}, \ldots, x_{n}$


\begin{definition}[coefficient $c_{k_{1} k_{2} \cdots k_{n}}$]
    coefficient $c_{k_{1} k_{2} \cdots k_{n}}$ is defined as
\begin{equation}
c_{k_{1} k_{2} \cdots k_{n}}=\frac{(d+1) !}{k_{0} ! k_{1} ! k_{2} ! \cdots k_{n} !} \quad \text { with } \quad k_{0}=d-k_{1}-\cdots-k_{n}
\end{equation}
\end{definition}



\section{Vector of monomials}

\begin{definition}
    write polynomial of degree $ d $ or less, with variables $ x \in \mathfrak{R}^{n} $, as
\begin{equation}
\hat{f}(x)=\theta^{T} F(x)
\end{equation}

$ F(x) $ is vector of basis functions
\begin{equation}
\sqrt{c_{k_{1}} \cdots k_{n}} x_{1}^{k_{1}} x_{2}^{k_{2}} \cdots x_{n}^{k_{n}} \quad \text { for all } k_{1}+k_{2}+\cdots+k_{n} \leq d
\end{equation}
\end{definition}

\begin{theorem}
    length of $ F(x) $ is $ p=(n+d) ! /(n ! d !) $
\end{theorem}

\begin{theorem}
    multinomial formula gives simple formula for inner products
    \begin{equation}
    \begin{aligned}
    F(u)^{T} F(v) &=\sum_{k_{1}+\cdots+k_{n} \leq d} c_{k_{1} k_{2} \cdots k_{n}}\left(u_{1}^{k_{1}} \cdots u_{n}^{k_{n}}\right)\left(v_{1}^{k_{1}} \cdots v_{n}^{k_{n}}\right) \\
    &=\left(1+u_{1} v_{1}+\cdots+u_{n} v_{n}\right)^{d}
    \end{aligned}
    \end{equation}
\end{theorem}


Only $ 2 n+1 $ flops needed for inner product of length $ p=(n+d) ! /(n ! d !) $.

\begin{example}
    vector of monomials of degree $ d=3 $ or less in $ n=2 $ variables

\begin{equation} F(u)^{T} F(v)=\left[\begin{array}{c}1 \\ \sqrt{3} u_{1} \\ \sqrt{3} u_{2} \\ \sqrt{3} u_{1}^{2} \\ \sqrt{6} u_{1} u_{2} \\ \sqrt{3} u_{2}^{2} \\ u_{1}^{3} \\ \sqrt{3} u_{1}^{2} u_{2} \\ \sqrt{3} u_{1} u_{2}^{2} \\ u_{2}^{3}\end{array}\right]^{T}\left[\begin{array}{c}1 \\ \sqrt{3} v_{1} \\ \sqrt{3} v_{2} \\ \sqrt{3} v_{1}^{2} \\ \sqrt{6} v_{1} v_{2} \\ \sqrt{3} v_{2}^{2} \\ v_{1}^{3} \\ \sqrt{3} v_{1}^{2} v_{2} \\ \sqrt{3} v_{1} v_{2}^{2} \\ v_{2}^{3}\end{array}\right] 
 =\left(1+u_{1} v_{1}+u_{2} v_{2}\right)^{3} \end{equation}
\end{example}



\section{Least squares fitting of multivariate polynomials}

\begin{problem}
    fit polynomial of $ n $ variables, degree $ \leq d $, to points $ \left(x^{(1)}, y^{(1)}\right), \ldots,\left(x^{(N)}, y^{(N)}\right) $ 
\end{problem}

\begin{algorithm}[htbp]
    \caption{Least squares fitting of multivariate polynomials}
    compute the $ N \times N $ matrix $ Q $ with elements
\begin{equation}
Q_{i j}=K\left(x^{(i)}, x^{(j)}\right) \text { where } K(u, v)=\left(1+u^{T} v\right)^{d}
\end{equation}\;
use a Cholesky factorization to solve the equation $ (Q+\lambda I) w=b $
\end{algorithm}

the fitted polynomial is
\begin{equation}
\hat{f}(x)=\sum_{i=1}^{N} w_{i} K\left(x^{(i)}, x\right)=\sum_{i=1}^{N} w_{i}\left(1+\left(x^{(i)}\right)^{T} x\right)^{d}
\end{equation}


complexity: $ n N^{2} $ flops for computing $ Q $, plus $ (1 / 3) N^{3} $ for the factorization, i.e.,
\begin{equation}
n N^{2}+(1 / 3) N^{3} \text { flops }
\end{equation}

\section{Kernel methods}

\begin{definition}[Kernel function]
    a generalized inner product $ K(u, v) $.
\end{definition}

$ K(u, v) $ is inner product of vectors of basis functions $ F(u) $ and $ F(v) $, $ F(u) $ may be infinite-dimensional.

Kernel methods work with $ K(u, v) $ directly, do not require $ F(u) $.

\begin{example}
    the polynomial kernel function $ K(u, v)=\left(1+u^{T} v\right)^{d} $
\end{example}

\begin{example}
    the Gaussian radial basis function kernel
\begin{equation}
K(u, v)=\exp \left(-\frac{\|u-v\|^{2}}{2 \sigma^{2}}\right)
\end{equation}
\end{example}

\begin{example}
    kernels exist for computing with graphs, texts, strings of symbols, ...
\end{example}


we apply the method  to least squares classification:

\begin{itemize}
    \item training set is 10000 images from MNIST data set ( $ \approx 1000 $ examples per digit)
    \item vector $ x $ is vector of pixel intensities (size $ n=28^{2}=784 $ )
    \item we use the polynomial kernel with degree $ d=3 $ :
\begin{equation}
K(u, v)=\left(1+u^{T} v\right)^{3}
\end{equation}

hence $ F(z) $ has length $ p=(n+d) ! /(n ! d !)=80931145 $

    \item we calculate ten Boolean classifiers
\begin{equation}
\hat{f}_{k}(x)=\operatorname{sign}\left(\tilde{f}_{k}(x)\right), \quad k=1, \ldots 10
\end{equation}
$ \hat{f}_{k}(x) $ distinguishes digit $ k-1 $ (outcome $ +1 $ ) form other digits (outcome -1)


\end{itemize}


the Boolean classifiers are combined in the multi-class classifier
\begin{equation}
\hat{f}(x)=\underset{k=1, \ldots, 10}{\operatorname{argmax}} \tilde{f}_{k}(x)
\end{equation}


\begin{algorithm}[htbp]
    \caption{compute Boolean classifier for digit $ k-1 $ versus the rest}
    compute $ N \times N $ matrix $ Q $ with elements
    \begin{equation}
    Q_{i j}=\left(1+\left(x^{(i)}\right)^{T} x^{(j)}\right)^{d}, \quad i, j=1, \ldots, N
    \end{equation}\;
    define $ N $-vector $ b=\left(y^{(1)}, \ldots, y^{(N)}\right) $ with elements
    \begin{equation}
    y^{(i)}=\left\{\begin{array}{ll}
    +1 & x^{(i)} \text { is an example of digit } k-1 \\
    -1 & \text { otherwise }
    \end{array}\right.
    \end{equation}\;
    solve the equation $ (Q+\lambda I) w=b $

\end{algorithm}

the solution $ w $ gives the Boolean classifier for digit $ k-1 $ versus rest
\begin{equation}
\tilde{f}_{k}(x)=\sum_{i=1}^{N} w_{i}\left(1+\left(x^{(i)}\right)^{T} x\right)^{d}
\end{equation}



The matrix $ Q $ is the same for each of the ten Boolean classifiers

Hence, only the right-hand side of the equation
\begin{equation}
(Q+\lambda I) w=y^{{d}}
\end{equation}
is different for each Boolean classifier.

\subsection{Complexity}

Complexity:

\begin{itemize}
    \item constructing $ Q $ requires $ N^{2} / 2 $ inner products of length $ n: n N^{2} $ flops
    \item Cholesky factorization of $ Q+\lambda I:(1 / 3) N^{3} $ flops
    \item solve the equation $ (Q+\lambda I) w=y^{{d}} $ for the 10 right-hand sides: $ 20 N^{2} $ flops
\end{itemize}

Total complexity is $ (1 / 3) N^{3}+n N^{2} $.

